{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    ":warning:**IMPORTANT NOTICE**:warning:\\\n",
    "*Since the initial parameterisation and gradient descent optimisation are stochastic processes, the training of a neural network is not fully reproducible.*\n",
    "\n",
    "*Therefore, it is not recommended to re-run this script as it will overwrite the original calibration of the neural network used in the work presented here.\n",
    "The purpose of this script is solely to document the training procedure and can be copied as a template to fit other new neural networks.*\n",
    "\n",
    "*To experiment with the models calibrated here, they can be loaded from the `saved_models` directory.*"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Systematic assesment of different hyperparameters to optimise model performance\n",
    "\n",
    "Various hyperparameters are varied to asses their impact on the model performance in a semi-quantitative way.\\\n",
    "The hyperparameters that are varied are:\n",
    "- Number of hidden layers / neurons --> model capacity\n",
    "- Initial learning rate\n",
    "- Activation function\n",
    "- Effect of regularization\n",
    "\n",
    "The model is trained on the same two datasets for each hyperparameter combination:\n",
    "1. Biotite composition (FMAST)\n",
    "2. Biotite composition (MnFMAST) + index minerals\n",
    "\n",
    "### Model capacity\n",
    "\n",
    "These architectures are tested:\n",
    "- **very small**: 1 hidden layers with 8 neurons each\n",
    "- **small**: 1 hidden layers with 16 neurons each\n",
    "- **large**: 1 hidden layers with 32 neurons each\n",
    "- **small_2hl**: 2 hidden layers with 16 neurons each\n",
    "- **large_2hl**: 2 hidden layers with 32 neurons each\n",
    "- **huge**: 3 hidden layers with 64 neurons\n",
    "\n",
    "### Learning rate\n",
    "\n",
    "These learning rates are tested:\n",
    "- 0.01\n",
    "- 0.001\n",
    "- 0.0005\n",
    "- 0.0001\n",
    "\n",
    "### Activation function\n",
    "*Decided against testing!*\n",
    "\n",
    "### Regularisation\n",
    "\n",
    "These regularisation methods are tested:\n",
    "- L2 regularisation\n",
    "- Dropout\n",
    "- Batch normalisation\n",
    "- Layer normalisation\n",
    "- Combined L2 regularisation and dropout\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\ThinkPad\\AppData\\Local\\Temp\\ipykernel_18372\\2311156246.py:2: DeprecationWarning: \n",
      "Pyarrow will become a required dependency of pandas in the next major release of pandas (pandas 3.0),\n",
      "(to allow more performant data types, such as the Arrow string type, and better interoperability with other libraries)\n",
      "but was not found to be installed on your system.\n",
      "If this would cause problems for you,\n",
      "please provide us feedback at https://github.com/pandas-dev/pandas/issues/54466\n",
      "        \n",
      "  import pandas as pd\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:From c:\\Users\\ThinkPad\\miniconda3\\envs\\masterproject\\Lib\\site-packages\\keras\\src\\losses.py:2976: The name tf.losses.sparse_softmax_cross_entropy is deprecated. Please use tf.compat.v1.losses.sparse_softmax_cross_entropy instead.\n",
      "\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "import tensorflow as tf\n",
    "\n",
    "from pathlib import Path\n",
    "from keras.models import Sequential\n",
    "from keras.layers import Dense, Normalization, BatchNormalization, LayerNormalization, Dropout\n",
    "from keras.losses import MeanSquaredError\n",
    "from keras.optimizers import Adam, schedules\n",
    "from keras.metrics import MeanAbsoluteError, RootMeanSquaredError\n",
    "from keras.callbacks import CSVLogger, EarlyStopping\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "from ml_tb.normalisation import MinMaxScaler\n",
    "from ml_tb.metrics import RMSE_denormalised_P, RMSE_denormalised_T"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Import data, pre-procesing and train/test split\n",
    "\n",
    "Load the two datasets and split them into training and validation sets.\n",
    "\n",
    "Validation set is **20%** of the training set. Approx. 200 samples."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "VALIDATION_FRACTION = 0.2"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "For the scaling of the P-T data, a MinMaxScaler is used.\n",
    "This scaler is defined globally and used for all datasets."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:From c:\\Users\\ThinkPad\\miniconda3\\envs\\masterproject\\Lib\\site-packages\\keras\\src\\backend.py:873: The name tf.get_default_graph is deprecated. Please use tf.compat.v1.get_default_graph instead.\n",
      "\n"
     ]
    }
   ],
   "source": [
    "scaling_pt = MinMaxScaler(min=[1500, 400], max=[10000, 900], axis=0)\n",
    "inv_scaling_pt = MinMaxScaler(min=[1500, 400], max=[10000, 900], axis=0, invert=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### **Dataset 1: Elements in biotite solution model (MnFMAST)**\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "NaN values in biotite composition:  False\n",
      "NaN values in PT:  False\n"
     ]
    }
   ],
   "source": [
    "# load excel file\n",
    "data = pd.read_excel(Path(\"Metapelite-Database_Bt_CLEAN_2024-02-03.xlsx\"))\n",
    "\n",
    "biotite_composition = np.zeros(shape=(len(data), 6))\n",
    "biotite_composition[:, 0] = data[\"Bt-Si\"]\n",
    "biotite_composition[:, 1] = data[\"Bt-Ti\"]\n",
    "biotite_composition[:, 2] = data[\"Bt-Al\"]\n",
    "biotite_composition[:, 3] = data[\"Bt-FeTot\"]\n",
    "biotite_composition[:, 4] = data[\"Bt-Mn\"]\n",
    "biotite_composition[:, 5] = data[\"Bt-Mg\"]\n",
    "\n",
    "pt = np.zeros(shape=(len(data), 2))\n",
    "pt[:, 0] = data[\"Pressure estimate random uniform\"] * 1000 # convert to bar\n",
    "pt[:, 1] = data[\"Temperature random ordered after Ti-in-Bt\"]\n",
    "\n",
    "# check for NaN values (should be already filtered out)\n",
    "print(\"NaN values in biotite composition: \", np.isnan(biotite_composition).any())\n",
    "print(\"NaN values in PT: \", np.isnan(pt).any())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[2.7081466  0.12082674 1.7106036  1.2264906  0.00852969 1.0918795 ]]\n",
      "[[0.04705625 0.04727941 0.09257054 0.22646412 0.00741701 0.2587144 ]]\n",
      "After normalisation, the minimal value of P and T is:  [0.00010304 0.00066034]\n",
      "After normalisation, the maximal value of P and T is:  [0.940462  0.8988128]\n"
     ]
    }
   ],
   "source": [
    "# test train split\n",
    "DATA01_biotite_composition_train, DATA01_biotite_composition_val, DATA01_pt_train, DATA01_pt_val = train_test_split(biotite_composition, pt, test_size=VALIDATION_FRACTION, shuffle=True)\n",
    "\n",
    "DATA01_normalisation_biotite_composition = Normalization(axis=-1)\n",
    "DATA01_normalisation_biotite_composition.adapt(DATA01_biotite_composition_train)\n",
    "\n",
    "print(DATA01_normalisation_biotite_composition.mean.numpy())\n",
    "print(np.sqrt(DATA01_normalisation_biotite_composition.variance.numpy()))\n",
    "\n",
    "DATA01_pt_train_norm = scaling_pt(DATA01_pt_train)\n",
    "DATA01_pt_val_norm = scaling_pt(DATA01_pt_val)\n",
    "\n",
    "print(\"After normalisation, the minimal value of P and T is: \", DATA01_pt_train_norm.numpy().min(axis=0))\n",
    "print(\"After normalisation, the maximal value of P and T is: \", DATA01_pt_train_norm.numpy().max(axis=0))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### **Dataset 2: Elements in biotite solution model + Index minerals**\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "NaN values in biotite composition & index minerals:  False\n",
      "NaN values in PT:  False\n"
     ]
    }
   ],
   "source": [
    "# load excel file\n",
    "data = pd.read_excel(Path(\"Metapelite-Database_Bt_CLEAN_2024-02-03.xlsx\"))\n",
    "\n",
    "biotite_composition = np.zeros(shape=(len(data), 6))\n",
    "biotite_composition[:, 0] = data[\"Bt-Si\"]\n",
    "biotite_composition[:, 1] = data[\"Bt-Ti\"]\n",
    "biotite_composition[:, 2] = data[\"Bt-Al\"]\n",
    "biotite_composition[:, 3] = data[\"Bt-FeTot\"]\n",
    "biotite_composition[:, 4] = data[\"Bt-Mn\"]\n",
    "biotite_composition[:, 5] = data[\"Bt-Mg\"]\n",
    "\n",
    "# extract one-hot encoded minerals in the following order: Chl, Grt, Crd, And, St, Ky, Sil, Kfs\n",
    "index_minerals = np.zeros(shape=(len(data), 8))\n",
    "index_minerals[:, 0] = data[\"Chl\"]\n",
    "index_minerals[:, 1] = data[\"Grt\"]\n",
    "index_minerals[:, 2] = data[\"Crd\"]\n",
    "index_minerals[:, 3] = data[\"And\"]\n",
    "index_minerals[:, 4] = data[\"St\"]\n",
    "index_minerals[:, 5] = data[\"Ky\"]\n",
    "index_minerals[:, 6] = data[\"Sil\"]\n",
    "index_minerals[:, 7] = data[\"Kfs\"]\n",
    "\n",
    "# Some minerals (Chl, Grt, St) have NaN values. Replace them with 0. Most likely samples with regional or metastable phases?!\n",
    "index_minerals = np.nan_to_num(index_minerals, nan=0)\n",
    "\n",
    "# combine biotite composition and one-hot encoded minerals\n",
    "biotite_composition_idxmin = np.concatenate((biotite_composition, index_minerals), axis=1)\n",
    "\n",
    "\n",
    "pt = np.zeros(shape=(len(data), 2))\n",
    "pt[:, 0] = data[\"Pressure estimate random uniform\"] * 1000 # convert to bar\n",
    "pt[:, 1] = data[\"Temperature random ordered after Ti-in-Bt\"]\n",
    "\n",
    "# check for NaN values (should be already filtered out)\n",
    "print(\"NaN values in biotite composition & index minerals: \", np.isnan(biotite_composition_idxmin).any())\n",
    "print(\"NaN values in PT: \", np.isnan(pt).any())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[2.7076685  0.12076807 1.7106233  1.2272779  0.00841155 1.0923196\n",
      "  0.         0.         0.         0.         0.         0.\n",
      "  0.         0.        ]]\n",
      "[[0.04731337 0.04801339 0.09337769 0.23137051 0.00724167 0.26396626\n",
      "  1.         1.         1.         1.         1.         1.\n",
      "  1.         1.        ]]\n",
      "After normalisation, the minimal value of P and T is:  [0.00010304 0.00066034]\n",
      "After normalisation, the maximal value of P and T is:  [0.940462  0.8988128]\n"
     ]
    }
   ],
   "source": [
    "# test train split\n",
    "DATA02_biotite_composition_train, DATA02_biotite_composition_val, DATA02_pt_train, DATA02_pt_val = train_test_split(biotite_composition_idxmin, pt, test_size=VALIDATION_FRACTION, shuffle=True)\n",
    "\n",
    "DATA02_normalisation_biotite_composition = Normalization(axis=-1)\n",
    "DATA02_normalisation_biotite_composition.adapt(DATA02_biotite_composition_train)\n",
    "\n",
    "# the one-hot encoded minerals should not be normalised. This is not necessary, as they are already in the range [0, 1]\n",
    "# Set the mean and variance to 0 and 1, respectively\n",
    "norm_mean = DATA02_normalisation_biotite_composition.mean.numpy()\n",
    "norm_mean[0,-8:] = 0\n",
    "norm_var = DATA02_normalisation_biotite_composition.variance.numpy()\n",
    "norm_var[0,-8:] = 1\n",
    "DATA02_normalisation_biotite_composition.mean = tf.convert_to_tensor(norm_mean)\n",
    "DATA02_normalisation_biotite_composition.variance = tf.convert_to_tensor(norm_var)\n",
    "\n",
    "print(DATA02_normalisation_biotite_composition.mean.numpy())\n",
    "print(np.sqrt(DATA02_normalisation_biotite_composition.variance.numpy()))\n",
    "\n",
    "DATA02_pt_train_norm = scaling_pt(DATA02_pt_train)\n",
    "DATA02_pt_val_norm = scaling_pt(DATA02_pt_val)\n",
    "\n",
    "print(\"After normalisation, the minimal value of P and T is: \", DATA02_pt_train_norm.numpy().min(axis=0))\n",
    "print(\"After normalisation, the maximal value of P and T is: \", DATA02_pt_train_norm.numpy().max(axis=0))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Set-up global training parameters\n",
    "\n",
    "Define a function to calculate RMSE for pressure and temperature for unscaled values to have an interpretable metric.\n",
    "\n",
    "All models are trained for a maximum of 5000 epochs.\\\n",
    "Early stopping is used with a patience (no improvement of val loss) of 50 epochs.\\\n",
    "Inverse time learning rate decay is used for all models."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "def RMSE_P(y_true, y_pred):\n",
    "    return RMSE_denormalised_P(y_true, y_pred, inv_scaling_pt)\n",
    "\n",
    "\n",
    "def RMSE_T(y_true, y_pred):\n",
    "    return RMSE_denormalised_T(y_true, y_pred, inv_scaling_pt)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "BATCH_SIZE = 50\n",
    "STEPS_PER_EPOCH = len(DATA01_biotite_composition_train) // BATCH_SIZE\n",
    "MAX_EPOCHS = 5000\n",
    "\n",
    "lr_schedule = schedules.InverseTimeDecay(0.001, decay_steps=STEPS_PER_EPOCH*1000, decay_rate=1, staircase=False)\n",
    "\n",
    "LOSS = MeanSquaredError()\n",
    "METRICS = [MeanAbsoluteError(), RootMeanSquaredError(), RMSE_P, RMSE_T]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## **Test 01:** Model capacity"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- **very small**: 1 hidden layers with 8 neurons each\n",
    "- **small**: 1 hidden layers with 16 neurons each\n",
    "- **large**: 1 hidden layers with 32 neurons each\n",
    "- **small_2hl**: 2 hidden layers with 16 neurons each\n",
    "- **large_2hl**: 2 hidden layers with 32 neurons each\n",
    "- **huge**: 3 hidden layers with 64 neurons"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "CALLBACKS_VERYSMALL_01 = [CSVLogger(\"HyperParamTest_verysmall_DATA01.log\", append=False), EarlyStopping(monitor=\"loss\", patience=50)]\n",
    "CALLBACKS_VERYSMALL_02 = [CSVLogger(\"HyperParamTest_verysmall_DATA02.log\", append=False), EarlyStopping(monitor=\"loss\", patience=50)]\n",
    "CALLBACKS_SMALL_01 = [CSVLogger(\"HyperParamTest_small_DATA01.log\", append=False), EarlyStopping(monitor=\"loss\", patience=50)]\n",
    "CALLBACKS_SMALL_02 = [CSVLogger(\"HyperParamTest_small_DATA02.log\", append=False), EarlyStopping(monitor=\"loss\", patience=50)]\n",
    "CALLBACKS_LARGE_01 = [CSVLogger(\"HyperParamTest_large_DATA01.log\", append=False), EarlyStopping(monitor=\"loss\", patience=50)]\n",
    "CALLBACKS_LARGE_02 = [CSVLogger(\"HyperParamTest_large_DATA02.log\", append=False), EarlyStopping(monitor=\"loss\", patience=50)]\n",
    "CALLBACKS_SMALL_2HL_01 = [CSVLogger(\"HyperParamTest_small_2HL_DATA01.log\", append=False), EarlyStopping(monitor=\"loss\", patience=50)]\n",
    "CALLBACKS_SMALL_2HL_02 = [CSVLogger(\"HyperParamTest_small_2HL_DATA02.log\", append=False), EarlyStopping(monitor=\"loss\", patience=50)]\n",
    "CALLBACKS_LARGE_2HL_01 = [CSVLogger(\"HyperParamTest_large_2HL_DATA01.log\", append=False), EarlyStopping(monitor=\"loss\", patience=50)]\n",
    "CALLBACKS_LARGE_2HL_02 = [CSVLogger(\"HyperParamTest_large_2HL_DATA02.log\", append=False), EarlyStopping(monitor=\"loss\", patience=50)]\n",
    "CALLBACKS_HUGE_01 = [CSVLogger(\"HyperParamTest_huge_DATA01.log\", append=False), EarlyStopping(monitor=\"loss\", patience=50)]\n",
    "CALLBACKS_HUGE_02 = [CSVLogger(\"HyperParamTest_huge_DATA02.log\", append=False), EarlyStopping(monitor=\"loss\", patience=50)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"sequential\"\n",
      "_________________________________________________________________\n",
      " Layer (type)                Output Shape              Param #   \n",
      "=================================================================\n",
      " normalization (Normalizati  (None, 6)                 13        \n",
      " on)                                                             \n",
      "                                                                 \n",
      " dense (Dense)               (None, 8)                 56        \n",
      "                                                                 \n",
      " dense_1 (Dense)             (None, 2)                 18        \n",
      "                                                                 \n",
      "=================================================================\n",
      "Total params: 87 (352.00 Byte)\n",
      "Trainable params: 74 (296.00 Byte)\n",
      "Non-trainable params: 13 (56.00 Byte)\n",
      "_________________________________________________________________\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:From c:\\Users\\ThinkPad\\miniconda3\\envs\\masterproject\\Lib\\site-packages\\keras\\src\\utils\\tf_utils.py:492: The name tf.ragged.RaggedTensorValue is deprecated. Please use tf.compat.v1.ragged.RaggedTensorValue instead.\n",
      "\n",
      "WARNING:tensorflow:From c:\\Users\\ThinkPad\\miniconda3\\envs\\masterproject\\Lib\\site-packages\\keras\\src\\engine\\base_layer_utils.py:384: The name tf.executing_eagerly_outside_functions is deprecated. Please use tf.compat.v1.executing_eagerly_outside_functions instead.\n",
      "\n",
      "INFO:tensorflow:Assets written to: saved_models\\verysmall_model_DATA01\\assets\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Assets written to: saved_models\\verysmall_model_DATA01\\assets\n"
     ]
    }
   ],
   "source": [
    "OPT = Adam(lr_schedule)\n",
    "\n",
    "verysmall_model = Sequential()\n",
    "verysmall_model.add(DATA01_normalisation_biotite_composition)\n",
    "verysmall_model.add(Dense(8, activation=\"relu\"))\n",
    "verysmall_model.add(Dense(2))\n",
    "\n",
    "verysmall_model.compile(optimizer=OPT, loss=LOSS, metrics=METRICS)\n",
    "verysmall_model.summary()\n",
    "\n",
    "history = verysmall_model.fit(DATA01_biotite_composition_train, DATA01_pt_train_norm,\n",
    "                          batch_size=BATCH_SIZE, epochs=MAX_EPOCHS,\n",
    "                          validation_data=[DATA01_biotite_composition_val, DATA01_pt_val_norm],\n",
    "                          callbacks=CALLBACKS_VERYSMALL_01, verbose=False)\n",
    "verysmall_model.save(Path(\"saved_models\", \"verysmall_model_DATA01\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"sequential_1\"\n",
      "_________________________________________________________________\n",
      " Layer (type)                Output Shape              Param #   \n",
      "=================================================================\n",
      " normalization_1 (Normaliza  (None, 14)                29        \n",
      " tion)                                                           \n",
      "                                                                 \n",
      " dense_2 (Dense)             (None, 8)                 120       \n",
      "                                                                 \n",
      " dense_3 (Dense)             (None, 2)                 18        \n",
      "                                                                 \n",
      "=================================================================\n",
      "Total params: 167 (672.00 Byte)\n",
      "Trainable params: 138 (552.00 Byte)\n",
      "Non-trainable params: 29 (120.00 Byte)\n",
      "_________________________________________________________________\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Assets written to: saved_models\\verysmall_model_DATA02\\assets\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Assets written to: saved_models\\verysmall_model_DATA02\\assets\n"
     ]
    }
   ],
   "source": [
    "OPT = Adam(lr_schedule)\n",
    "\n",
    "verysmall_model = Sequential()\n",
    "verysmall_model.add(DATA02_normalisation_biotite_composition)\n",
    "verysmall_model.add(Dense(8, activation=\"relu\"))\n",
    "verysmall_model.add(Dense(2))\n",
    "\n",
    "verysmall_model.compile(optimizer=OPT, loss=LOSS, metrics=METRICS)\n",
    "verysmall_model.summary()\n",
    "\n",
    "history = verysmall_model.fit(DATA02_biotite_composition_train, DATA02_pt_train_norm,\n",
    "                          batch_size=BATCH_SIZE, epochs=MAX_EPOCHS,\n",
    "                          validation_data=[DATA02_biotite_composition_val, DATA02_pt_val_norm],\n",
    "                          callbacks=CALLBACKS_VERYSMALL_02, verbose=False)\n",
    "verysmall_model.save(Path(\"saved_models\", \"verysmall_model_DATA02\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"sequential_2\"\n",
      "_________________________________________________________________\n",
      " Layer (type)                Output Shape              Param #   \n",
      "=================================================================\n",
      " normalization (Normalizati  (None, 6)                 13        \n",
      " on)                                                             \n",
      "                                                                 \n",
      " dense_4 (Dense)             (None, 16)                112       \n",
      "                                                                 \n",
      " dense_5 (Dense)             (None, 2)                 34        \n",
      "                                                                 \n",
      "=================================================================\n",
      "Total params: 159 (640.00 Byte)\n",
      "Trainable params: 146 (584.00 Byte)\n",
      "Non-trainable params: 13 (56.00 Byte)\n",
      "_________________________________________________________________\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Assets written to: saved_models\\small_model_DATA01\\assets\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Assets written to: saved_models\\small_model_DATA01\\assets\n"
     ]
    }
   ],
   "source": [
    "OPT = Adam(lr_schedule)\n",
    "\n",
    "small_model = Sequential()\n",
    "small_model.add(DATA01_normalisation_biotite_composition)\n",
    "small_model.add(Dense(16, activation=\"relu\"))\n",
    "small_model.add(Dense(2))\n",
    "\n",
    "small_model.compile(optimizer=OPT, loss=LOSS, metrics=METRICS)\n",
    "small_model.summary()\n",
    "\n",
    "history = small_model.fit(DATA01_biotite_composition_train, DATA01_pt_train_norm,\n",
    "                          batch_size=BATCH_SIZE, epochs=MAX_EPOCHS,\n",
    "                          validation_data=[DATA01_biotite_composition_val, DATA01_pt_val_norm],\n",
    "                          callbacks=CALLBACKS_SMALL_01, verbose=False)\n",
    "small_model.save(Path(\"saved_models\", \"small_model_DATA01\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"sequential_3\"\n",
      "_________________________________________________________________\n",
      " Layer (type)                Output Shape              Param #   \n",
      "=================================================================\n",
      " normalization_1 (Normaliza  (None, 14)                29        \n",
      " tion)                                                           \n",
      "                                                                 \n",
      " dense_6 (Dense)             (None, 16)                240       \n",
      "                                                                 \n",
      " dense_7 (Dense)             (None, 2)                 34        \n",
      "                                                                 \n",
      "=================================================================\n",
      "Total params: 303 (1.19 KB)\n",
      "Trainable params: 274 (1.07 KB)\n",
      "Non-trainable params: 29 (120.00 Byte)\n",
      "_________________________________________________________________\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Assets written to: saved_models\\small_model_DATA02\\assets\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Assets written to: saved_models\\small_model_DATA02\\assets\n"
     ]
    }
   ],
   "source": [
    "OPT = Adam(lr_schedule)\n",
    "\n",
    "small_model = Sequential()\n",
    "small_model.add(DATA02_normalisation_biotite_composition)\n",
    "small_model.add(Dense(16, activation=\"relu\"))\n",
    "small_model.add(Dense(2))\n",
    "\n",
    "small_model.compile(optimizer=OPT, loss=LOSS, metrics=METRICS)\n",
    "small_model.summary()\n",
    "\n",
    "history = small_model.fit(DATA02_biotite_composition_train, DATA02_pt_train_norm,\n",
    "                          batch_size=BATCH_SIZE, epochs=MAX_EPOCHS,\n",
    "                          validation_data=[DATA02_biotite_composition_val, DATA02_pt_val_norm],\n",
    "                          callbacks=CALLBACKS_SMALL_02, verbose=False)\n",
    "small_model.save(Path(\"saved_models\", \"small_model_DATA02\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"sequential_4\"\n",
      "_________________________________________________________________\n",
      " Layer (type)                Output Shape              Param #   \n",
      "=================================================================\n",
      " normalization (Normalizati  (None, 6)                 13        \n",
      " on)                                                             \n",
      "                                                                 \n",
      " dense_8 (Dense)             (None, 16)                112       \n",
      "                                                                 \n",
      " dense_9 (Dense)             (None, 16)                272       \n",
      "                                                                 \n",
      " dense_10 (Dense)            (None, 2)                 34        \n",
      "                                                                 \n",
      "=================================================================\n",
      "Total params: 431 (1.69 KB)\n",
      "Trainable params: 418 (1.63 KB)\n",
      "Non-trainable params: 13 (56.00 Byte)\n",
      "_________________________________________________________________\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Assets written to: saved_models\\small_2hl_model_DATA01\\assets\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Assets written to: saved_models\\small_2hl_model_DATA01\\assets\n"
     ]
    }
   ],
   "source": [
    "OPT = Adam(lr_schedule)\n",
    "\n",
    "small_2hl_model = Sequential()\n",
    "small_2hl_model.add(DATA01_normalisation_biotite_composition)\n",
    "small_2hl_model.add(Dense(16, activation=\"relu\"))\n",
    "small_2hl_model.add(Dense(16, activation=\"relu\"))\n",
    "small_2hl_model.add(Dense(2))\n",
    "\n",
    "small_2hl_model.compile(optimizer=OPT, loss=LOSS, metrics=METRICS)\n",
    "small_2hl_model.summary()\n",
    "\n",
    "history = small_2hl_model.fit(DATA01_biotite_composition_train, DATA01_pt_train_norm,\n",
    "                          batch_size=BATCH_SIZE, epochs=MAX_EPOCHS,\n",
    "                          validation_data=[DATA01_biotite_composition_val, DATA01_pt_val_norm],\n",
    "                          callbacks=CALLBACKS_SMALL_2HL_01, verbose=False)\n",
    "small_2hl_model.save(Path(\"saved_models\", \"small_2hl_model_DATA01\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"sequential_5\"\n",
      "_________________________________________________________________\n",
      " Layer (type)                Output Shape              Param #   \n",
      "=================================================================\n",
      " normalization_1 (Normaliza  (None, 14)                29        \n",
      " tion)                                                           \n",
      "                                                                 \n",
      " dense_11 (Dense)            (None, 16)                240       \n",
      "                                                                 \n",
      " dense_12 (Dense)            (None, 16)                272       \n",
      "                                                                 \n",
      " dense_13 (Dense)            (None, 2)                 34        \n",
      "                                                                 \n",
      "=================================================================\n",
      "Total params: 575 (2.25 KB)\n",
      "Trainable params: 546 (2.13 KB)\n",
      "Non-trainable params: 29 (120.00 Byte)\n",
      "_________________________________________________________________\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Assets written to: saved_models\\small_2hl_model_DATA02\\assets\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Assets written to: saved_models\\small_2hl_model_DATA02\\assets\n"
     ]
    }
   ],
   "source": [
    "OPT = Adam(lr_schedule)\n",
    "\n",
    "small_2hl_model = Sequential()\n",
    "small_2hl_model.add(DATA02_normalisation_biotite_composition)\n",
    "small_2hl_model.add(Dense(16, activation=\"relu\"))\n",
    "small_2hl_model.add(Dense(16, activation=\"relu\"))\n",
    "small_2hl_model.add(Dense(2))\n",
    "\n",
    "small_2hl_model.compile(optimizer=OPT, loss=LOSS, metrics=METRICS)\n",
    "small_2hl_model.summary()\n",
    "\n",
    "history = small_2hl_model.fit(DATA02_biotite_composition_train, DATA02_pt_train_norm,\n",
    "                          batch_size=BATCH_SIZE, epochs=MAX_EPOCHS,\n",
    "                          validation_data=[DATA02_biotite_composition_val, DATA02_pt_val_norm],\n",
    "                          callbacks=CALLBACKS_SMALL_2HL_02, verbose=False)\n",
    "small_2hl_model.save(Path(\"saved_models\", \"small_2hl_model_DATA02\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"sequential_6\"\n",
      "_________________________________________________________________\n",
      " Layer (type)                Output Shape              Param #   \n",
      "=================================================================\n",
      " normalization (Normalizati  (None, 6)                 13        \n",
      " on)                                                             \n",
      "                                                                 \n",
      " dense_14 (Dense)            (None, 32)                224       \n",
      "                                                                 \n",
      " dense_15 (Dense)            (None, 2)                 66        \n",
      "                                                                 \n",
      "=================================================================\n",
      "Total params: 303 (1.19 KB)\n",
      "Trainable params: 290 (1.13 KB)\n",
      "Non-trainable params: 13 (56.00 Byte)\n",
      "_________________________________________________________________\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Assets written to: saved_models\\large_model_DATA01\\assets\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Assets written to: saved_models\\large_model_DATA01\\assets\n"
     ]
    }
   ],
   "source": [
    "OPT = Adam(lr_schedule)\n",
    "\n",
    "large_model = Sequential()\n",
    "large_model.add(DATA01_normalisation_biotite_composition)\n",
    "large_model.add(Dense(32, activation=\"relu\"))\n",
    "large_model.add(Dense(2))\n",
    "\n",
    "large_model.compile(optimizer=OPT, loss=LOSS, metrics=METRICS)\n",
    "large_model.summary()\n",
    "\n",
    "history = large_model.fit(DATA01_biotite_composition_train, DATA01_pt_train_norm,\n",
    "                          batch_size=BATCH_SIZE, epochs=MAX_EPOCHS,\n",
    "                          validation_data=[DATA01_biotite_composition_val, DATA01_pt_val_norm],\n",
    "                          callbacks=CALLBACKS_LARGE_01, verbose=False)\n",
    "large_model.save(Path(\"saved_models\", \"large_model_DATA01\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"sequential_7\"\n",
      "_________________________________________________________________\n",
      " Layer (type)                Output Shape              Param #   \n",
      "=================================================================\n",
      " normalization_1 (Normaliza  (None, 14)                29        \n",
      " tion)                                                           \n",
      "                                                                 \n",
      " dense_16 (Dense)            (None, 32)                480       \n",
      "                                                                 \n",
      " dense_17 (Dense)            (None, 2)                 66        \n",
      "                                                                 \n",
      "=================================================================\n",
      "Total params: 575 (2.25 KB)\n",
      "Trainable params: 546 (2.13 KB)\n",
      "Non-trainable params: 29 (120.00 Byte)\n",
      "_________________________________________________________________\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Assets written to: saved_models\\large_model_DATA02\\assets\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Assets written to: saved_models\\large_model_DATA02\\assets\n"
     ]
    }
   ],
   "source": [
    "OPT = Adam(lr_schedule)\n",
    "\n",
    "large_model = Sequential()\n",
    "large_model.add(DATA02_normalisation_biotite_composition)\n",
    "large_model.add(Dense(32, activation=\"relu\"))\n",
    "large_model.add(Dense(2))\n",
    "\n",
    "large_model.compile(optimizer=OPT, loss=LOSS, metrics=METRICS)\n",
    "large_model.summary()\n",
    "\n",
    "history = large_model.fit(DATA02_biotite_composition_train, DATA02_pt_train_norm,\n",
    "                          batch_size=BATCH_SIZE, epochs=MAX_EPOCHS,\n",
    "                          validation_data=[DATA02_biotite_composition_val, DATA02_pt_val_norm],\n",
    "                          callbacks=CALLBACKS_LARGE_02, verbose=False)\n",
    "large_model.save(Path(\"saved_models\", \"large_model_DATA02\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"sequential_8\"\n",
      "_________________________________________________________________\n",
      " Layer (type)                Output Shape              Param #   \n",
      "=================================================================\n",
      " normalization (Normalizati  (None, 6)                 13        \n",
      " on)                                                             \n",
      "                                                                 \n",
      " dense_18 (Dense)            (None, 32)                224       \n",
      "                                                                 \n",
      " dense_19 (Dense)            (None, 2)                 66        \n",
      "                                                                 \n",
      "=================================================================\n",
      "Total params: 303 (1.19 KB)\n",
      "Trainable params: 290 (1.13 KB)\n",
      "Non-trainable params: 13 (56.00 Byte)\n",
      "_________________________________________________________________\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Assets written to: saved_models\\large_2hl_model_DATA01\\assets\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Assets written to: saved_models\\large_2hl_model_DATA01\\assets\n"
     ]
    }
   ],
   "source": [
    "OPT = Adam(lr_schedule)\n",
    "\n",
    "large_2hl_model = Sequential()\n",
    "large_2hl_model.add(DATA01_normalisation_biotite_composition)\n",
    "large_2hl_model.add(Dense(32, activation=\"relu\"))\n",
    "large_2hl_model.add(Dense(2))\n",
    "\n",
    "large_2hl_model.compile(optimizer=OPT, loss=LOSS, metrics=METRICS)\n",
    "large_2hl_model.summary()\n",
    "\n",
    "history = large_2hl_model.fit(DATA01_biotite_composition_train, DATA01_pt_train_norm,\n",
    "                          batch_size=BATCH_SIZE, epochs=MAX_EPOCHS,\n",
    "                          validation_data=[DATA01_biotite_composition_val, DATA01_pt_val_norm],\n",
    "                          callbacks=CALLBACKS_LARGE_2HL_01, verbose=False)\n",
    "large_2hl_model.save(Path(\"saved_models\", \"large_2hl_model_DATA01\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"sequential_9\"\n",
      "_________________________________________________________________\n",
      " Layer (type)                Output Shape              Param #   \n",
      "=================================================================\n",
      " normalization_1 (Normaliza  (None, 14)                29        \n",
      " tion)                                                           \n",
      "                                                                 \n",
      " dense_20 (Dense)            (None, 32)                480       \n",
      "                                                                 \n",
      " dense_21 (Dense)            (None, 2)                 66        \n",
      "                                                                 \n",
      "=================================================================\n",
      "Total params: 575 (2.25 KB)\n",
      "Trainable params: 546 (2.13 KB)\n",
      "Non-trainable params: 29 (120.00 Byte)\n",
      "_________________________________________________________________\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Assets written to: saved_models\\large_2hl_model_DATA02\\assets\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Assets written to: saved_models\\large_2hl_model_DATA02\\assets\n"
     ]
    }
   ],
   "source": [
    "OPT = Adam(lr_schedule)\n",
    "\n",
    "large_2hl_model = Sequential()\n",
    "large_2hl_model.add(DATA02_normalisation_biotite_composition)\n",
    "large_2hl_model.add(Dense(32, activation=\"relu\"))\n",
    "large_2hl_model.add(Dense(2))\n",
    "\n",
    "large_2hl_model.compile(optimizer=OPT, loss=LOSS, metrics=METRICS)\n",
    "large_2hl_model.summary()\n",
    "\n",
    "history = large_2hl_model.fit(DATA02_biotite_composition_train, DATA02_pt_train_norm,\n",
    "                          batch_size=BATCH_SIZE, epochs=MAX_EPOCHS,\n",
    "                          validation_data=[DATA02_biotite_composition_val, DATA02_pt_val_norm],\n",
    "                          callbacks=CALLBACKS_LARGE_2HL_02, verbose=False)\n",
    "large_2hl_model.save(Path(\"saved_models\", \"large_2hl_model_DATA02\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"sequential_10\"\n",
      "_________________________________________________________________\n",
      " Layer (type)                Output Shape              Param #   \n",
      "=================================================================\n",
      " normalization (Normalizati  (None, 6)                 13        \n",
      " on)                                                             \n",
      "                                                                 \n",
      " dense_22 (Dense)            (None, 64)                448       \n",
      "                                                                 \n",
      " dense_23 (Dense)            (None, 64)                4160      \n",
      "                                                                 \n",
      " dense_24 (Dense)            (None, 64)                4160      \n",
      "                                                                 \n",
      " dense_25 (Dense)            (None, 2)                 130       \n",
      "                                                                 \n",
      "=================================================================\n",
      "Total params: 8911 (34.81 KB)\n",
      "Trainable params: 8898 (34.76 KB)\n",
      "Non-trainable params: 13 (56.00 Byte)\n",
      "_________________________________________________________________\n",
      "INFO:tensorflow:Assets written to: saved_models\\huge_model_DATA01\\assets\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Assets written to: saved_models\\huge_model_DATA01\\assets\n"
     ]
    }
   ],
   "source": [
    "OPT = Adam(lr_schedule)\n",
    "\n",
    "huge_model = Sequential()\n",
    "huge_model.add(DATA01_normalisation_biotite_composition)\n",
    "huge_model.add(Dense(64, activation=\"relu\"))\n",
    "huge_model.add(Dense(64, activation=\"relu\"))\n",
    "huge_model.add(Dense(64, activation=\"relu\"))\n",
    "huge_model.add(Dense(2))\n",
    "\n",
    "huge_model.compile(optimizer=OPT, loss=LOSS, metrics=METRICS)\n",
    "huge_model.summary()\n",
    "\n",
    "history = huge_model.fit(DATA01_biotite_composition_train, DATA01_pt_train_norm,\n",
    "                          batch_size=BATCH_SIZE, epochs=MAX_EPOCHS,\n",
    "                          validation_data=[DATA01_biotite_composition_val, DATA01_pt_val_norm],\n",
    "                          callbacks=CALLBACKS_HUGE_01, verbose=False)\n",
    "huge_model.save(Path(\"saved_models\", \"huge_model_DATA01\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"sequential_11\"\n",
      "_________________________________________________________________\n",
      " Layer (type)                Output Shape              Param #   \n",
      "=================================================================\n",
      " normalization_1 (Normaliza  (None, 14)                29        \n",
      " tion)                                                           \n",
      "                                                                 \n",
      " dense_26 (Dense)            (None, 64)                960       \n",
      "                                                                 \n",
      " dense_27 (Dense)            (None, 64)                4160      \n",
      "                                                                 \n",
      " dense_28 (Dense)            (None, 64)                4160      \n",
      "                                                                 \n",
      " dense_29 (Dense)            (None, 2)                 130       \n",
      "                                                                 \n",
      "=================================================================\n",
      "Total params: 9439 (36.88 KB)\n",
      "Trainable params: 9410 (36.76 KB)\n",
      "Non-trainable params: 29 (120.00 Byte)\n",
      "_________________________________________________________________\n",
      "INFO:tensorflow:Assets written to: saved_models\\huge_model_DATA02\\assets\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Assets written to: saved_models\\huge_model_DATA02\\assets\n"
     ]
    }
   ],
   "source": [
    "OPT = Adam(lr_schedule)\n",
    "\n",
    "huge_model = Sequential()\n",
    "huge_model.add(DATA02_normalisation_biotite_composition)\n",
    "huge_model.add(Dense(64, activation=\"relu\"))\n",
    "huge_model.add(Dense(64, activation=\"relu\"))\n",
    "huge_model.add(Dense(64, activation=\"relu\"))\n",
    "huge_model.add(Dense(2))\n",
    "\n",
    "huge_model.compile(optimizer=OPT, loss=LOSS, metrics=METRICS)\n",
    "huge_model.summary()\n",
    "\n",
    "history = huge_model.fit(DATA02_biotite_composition_train, DATA02_pt_train_norm,\n",
    "                          batch_size=BATCH_SIZE, epochs=MAX_EPOCHS,\n",
    "                          validation_data=[DATA02_biotite_composition_val, DATA02_pt_val_norm],\n",
    "                          callbacks=CALLBACKS_HUGE_02, verbose=False)\n",
    "huge_model.save(Path(\"saved_models\", \"huge_model_DATA02\"))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## **Test 02:** Learning rate\n",
    "\n",
    "All tests are performed with the optimal model architecture from Test 01. --> *small*\n",
    "\n",
    "These learning rates are tested:\n",
    "- 0.01\n",
    "- 0.001\n",
    "- 0.0005\n",
    "- 0.0001\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "CALLBACKS_LR1_01 = [CSVLogger(\"HyperParamTest_LR1_DATA01.log\", append=False), EarlyStopping(monitor=\"loss\", patience=50)]\n",
    "CALLBACKS_LR1_02 = [CSVLogger(\"HyperParamTest_LR1_DATA02.log\", append=False), EarlyStopping(monitor=\"loss\", patience=50)]\n",
    "CALLBACKS_LR2_01 = [CSVLogger(\"HyperParamTest_LR2_DATA01.log\", append=False), EarlyStopping(monitor=\"loss\", patience=50)]\n",
    "CALLBACKS_LR2_02 = [CSVLogger(\"HyperParamTest_LR2_DATA02.log\", append=False), EarlyStopping(monitor=\"loss\", patience=50)]\n",
    "CALLBACKS_LR3_01 = [CSVLogger(\"HyperParamTest_LR3_DATA01.log\", append=False), EarlyStopping(monitor=\"loss\", patience=50)]\n",
    "CALLBACKS_LR3_02 = [CSVLogger(\"HyperParamTest_LR3_DATA02.log\", append=False), EarlyStopping(monitor=\"loss\", patience=50)]\n",
    "CALLBACKS_LR4_01 = [CSVLogger(\"HyperParamTest_LR4_DATA01.log\", append=False), EarlyStopping(monitor=\"loss\", patience=50)]\n",
    "CALLBACKS_LR4_02 = [CSVLogger(\"HyperParamTest_LR4_DATA02.log\", append=False), EarlyStopping(monitor=\"loss\", patience=50)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "lr_schedule.initial_learning_rate = 0.01"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"sequential_12\"\n",
      "_________________________________________________________________\n",
      " Layer (type)                Output Shape              Param #   \n",
      "=================================================================\n",
      " normalization (Normalizati  (None, 6)                 13        \n",
      " on)                                                             \n",
      "                                                                 \n",
      " dense_30 (Dense)            (None, 16)                112       \n",
      "                                                                 \n",
      " dense_31 (Dense)            (None, 2)                 34        \n",
      "                                                                 \n",
      "=================================================================\n",
      "Total params: 159 (640.00 Byte)\n",
      "Trainable params: 146 (584.00 Byte)\n",
      "Non-trainable params: 13 (56.00 Byte)\n",
      "_________________________________________________________________\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Assets written to: saved_models\\LR1_model_DATA01\\assets\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Assets written to: saved_models\\LR1_model_DATA01\\assets\n"
     ]
    }
   ],
   "source": [
    "OPT = Adam(lr_schedule)\n",
    "\n",
    "LR1_model = Sequential()\n",
    "LR1_model.add(DATA01_normalisation_biotite_composition)\n",
    "LR1_model.add(Dense(16, activation=\"relu\"))\n",
    "LR1_model.add(Dense(2))\n",
    "\n",
    "LR1_model.compile(optimizer=OPT, loss=LOSS, metrics=METRICS)\n",
    "LR1_model.summary()\n",
    "\n",
    "history = LR1_model.fit(DATA01_biotite_composition_train, DATA01_pt_train_norm,\n",
    "                          batch_size=BATCH_SIZE, epochs=MAX_EPOCHS,\n",
    "                          validation_data=[DATA01_biotite_composition_val, DATA01_pt_val_norm],\n",
    "                          callbacks=CALLBACKS_LR1_01, verbose=False)\n",
    "LR1_model.save(Path(\"saved_models\", \"LR1_model_DATA01\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"sequential_13\"\n",
      "_________________________________________________________________\n",
      " Layer (type)                Output Shape              Param #   \n",
      "=================================================================\n",
      " normalization_1 (Normaliza  (None, 14)                29        \n",
      " tion)                                                           \n",
      "                                                                 \n",
      " dense_32 (Dense)            (None, 16)                240       \n",
      "                                                                 \n",
      " dense_33 (Dense)            (None, 2)                 34        \n",
      "                                                                 \n",
      "=================================================================\n",
      "Total params: 303 (1.19 KB)\n",
      "Trainable params: 274 (1.07 KB)\n",
      "Non-trainable params: 29 (120.00 Byte)\n",
      "_________________________________________________________________\n",
      "INFO:tensorflow:Assets written to: saved_models\\LR1_model_DATA02\\assets\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Assets written to: saved_models\\LR1_model_DATA02\\assets\n"
     ]
    }
   ],
   "source": [
    "OPT = Adam(lr_schedule)\n",
    "\n",
    "LR1_model = Sequential()\n",
    "LR1_model.add(DATA02_normalisation_biotite_composition)\n",
    "LR1_model.add(Dense(16, activation=\"relu\"))\n",
    "LR1_model.add(Dense(2))\n",
    "\n",
    "LR1_model.compile(optimizer=OPT, loss=LOSS, metrics=METRICS)\n",
    "LR1_model.summary()\n",
    "\n",
    "history = LR1_model.fit(DATA02_biotite_composition_train, DATA02_pt_train_norm,\n",
    "                        batch_size=BATCH_SIZE, epochs=MAX_EPOCHS,\n",
    "                        validation_data=[DATA02_biotite_composition_val, DATA02_pt_val_norm],\n",
    "                        callbacks=CALLBACKS_LR1_02, verbose=False)\n",
    "LR1_model.save(Path(\"saved_models\", \"LR1_model_DATA02\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "lr_schedule.initial_learning_rate = 0.001"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"sequential_14\"\n",
      "_________________________________________________________________\n",
      " Layer (type)                Output Shape              Param #   \n",
      "=================================================================\n",
      " normalization (Normalizati  (None, 6)                 13        \n",
      " on)                                                             \n",
      "                                                                 \n",
      " dense_34 (Dense)            (None, 16)                112       \n",
      "                                                                 \n",
      " dense_35 (Dense)            (None, 2)                 34        \n",
      "                                                                 \n",
      "=================================================================\n",
      "Total params: 159 (640.00 Byte)\n",
      "Trainable params: 146 (584.00 Byte)\n",
      "Non-trainable params: 13 (56.00 Byte)\n",
      "_________________________________________________________________\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Assets written to: saved_models\\LR2_model_DATA01\\assets\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Assets written to: saved_models\\LR2_model_DATA01\\assets\n"
     ]
    }
   ],
   "source": [
    "OPT = Adam(lr_schedule)\n",
    "\n",
    "LR2_model = Sequential()\n",
    "LR2_model.add(DATA01_normalisation_biotite_composition)\n",
    "LR2_model.add(Dense(16, activation=\"relu\"))\n",
    "LR2_model.add(Dense(2))\n",
    "\n",
    "LR2_model.compile(optimizer=OPT, loss=LOSS, metrics=METRICS)\n",
    "LR2_model.summary()\n",
    "\n",
    "history = LR2_model.fit(DATA01_biotite_composition_train, DATA01_pt_train_norm,\n",
    "                        batch_size=BATCH_SIZE, epochs=MAX_EPOCHS,\n",
    "                        validation_data=[DATA01_biotite_composition_val, DATA01_pt_val_norm],\n",
    "                        callbacks=CALLBACKS_LR2_01, verbose=False)\n",
    "\n",
    "LR2_model.save(Path(\"saved_models\", \"LR2_model_DATA01\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"sequential_15\"\n",
      "_________________________________________________________________\n",
      " Layer (type)                Output Shape              Param #   \n",
      "=================================================================\n",
      " normalization_1 (Normaliza  (None, 14)                29        \n",
      " tion)                                                           \n",
      "                                                                 \n",
      " dense_36 (Dense)            (None, 16)                240       \n",
      "                                                                 \n",
      " dense_37 (Dense)            (None, 2)                 34        \n",
      "                                                                 \n",
      "=================================================================\n",
      "Total params: 303 (1.19 KB)\n",
      "Trainable params: 274 (1.07 KB)\n",
      "Non-trainable params: 29 (120.00 Byte)\n",
      "_________________________________________________________________\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Assets written to: saved_models\\LR2_model_DATA02\\assets\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Assets written to: saved_models\\LR2_model_DATA02\\assets\n"
     ]
    }
   ],
   "source": [
    "OPT = Adam(lr_schedule)\n",
    "\n",
    "LR2_model = Sequential()\n",
    "LR2_model.add(DATA02_normalisation_biotite_composition)\n",
    "LR2_model.add(Dense(16, activation=\"relu\"))\n",
    "LR2_model.add(Dense(2))\n",
    "\n",
    "LR2_model.compile(optimizer=OPT, loss=LOSS, metrics=METRICS)\n",
    "LR2_model.summary()\n",
    "\n",
    "history = LR2_model.fit(DATA02_biotite_composition_train, DATA02_pt_train_norm,\n",
    "                        batch_size=BATCH_SIZE, epochs=MAX_EPOCHS,\n",
    "                        validation_data=[DATA02_biotite_composition_val, DATA02_pt_val_norm],\n",
    "                        callbacks=CALLBACKS_LR2_02, verbose=False)\n",
    "\n",
    "LR2_model.save(Path(\"saved_models\", \"LR2_model_DATA02\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "lr_schedule.initial_learning_rate = 0.0005"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"sequential_16\"\n",
      "_________________________________________________________________\n",
      " Layer (type)                Output Shape              Param #   \n",
      "=================================================================\n",
      " normalization (Normalizati  (None, 6)                 13        \n",
      " on)                                                             \n",
      "                                                                 \n",
      " dense_38 (Dense)            (None, 16)                112       \n",
      "                                                                 \n",
      " dense_39 (Dense)            (None, 2)                 34        \n",
      "                                                                 \n",
      "=================================================================\n",
      "Total params: 159 (640.00 Byte)\n",
      "Trainable params: 146 (584.00 Byte)\n",
      "Non-trainable params: 13 (56.00 Byte)\n",
      "_________________________________________________________________\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Assets written to: saved_models\\LR3_model_DATA01\\assets\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Assets written to: saved_models\\LR3_model_DATA01\\assets\n"
     ]
    }
   ],
   "source": [
    "OPT = Adam(lr_schedule)\n",
    "\n",
    "LR3_model = Sequential()\n",
    "LR3_model.add(DATA01_normalisation_biotite_composition)\n",
    "LR3_model.add(Dense(16, activation=\"relu\"))\n",
    "LR3_model.add(Dense(2))\n",
    "\n",
    "LR3_model.compile(optimizer=OPT, loss=LOSS, metrics=METRICS)\n",
    "LR3_model.summary()\n",
    "\n",
    "history = LR3_model.fit(DATA01_biotite_composition_train, DATA01_pt_train_norm,\n",
    "                        batch_size=BATCH_SIZE, epochs=MAX_EPOCHS,\n",
    "                        validation_data=[DATA01_biotite_composition_val, DATA01_pt_val_norm],\n",
    "                        callbacks=CALLBACKS_LR3_01, verbose=False)\n",
    "\n",
    "LR3_model.save(Path(\"saved_models\", \"LR3_model_DATA01\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"sequential_17\"\n",
      "_________________________________________________________________\n",
      " Layer (type)                Output Shape              Param #   \n",
      "=================================================================\n",
      " normalization_1 (Normaliza  (None, 14)                29        \n",
      " tion)                                                           \n",
      "                                                                 \n",
      " dense_40 (Dense)            (None, 16)                240       \n",
      "                                                                 \n",
      " dense_41 (Dense)            (None, 2)                 34        \n",
      "                                                                 \n",
      "=================================================================\n",
      "Total params: 303 (1.19 KB)\n",
      "Trainable params: 274 (1.07 KB)\n",
      "Non-trainable params: 29 (120.00 Byte)\n",
      "_________________________________________________________________\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Assets written to: saved_models\\LR3_model_DATA02\\assets\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Assets written to: saved_models\\LR3_model_DATA02\\assets\n"
     ]
    }
   ],
   "source": [
    "OPT = Adam(lr_schedule)\n",
    "\n",
    "LR3_model = Sequential()\n",
    "LR3_model.add(DATA02_normalisation_biotite_composition)\n",
    "LR3_model.add(Dense(16, activation=\"relu\"))\n",
    "LR3_model.add(Dense(2))\n",
    "\n",
    "LR3_model.compile(optimizer=OPT, loss=LOSS, metrics=METRICS)\n",
    "LR3_model.summary()\n",
    "\n",
    "history = LR3_model.fit(DATA02_biotite_composition_train, DATA02_pt_train_norm,\n",
    "                        batch_size=BATCH_SIZE, epochs=MAX_EPOCHS,\n",
    "                        validation_data=[DATA02_biotite_composition_val, DATA02_pt_val_norm],\n",
    "                        callbacks=CALLBACKS_LR3_02, verbose=False)\n",
    "\n",
    "LR3_model.save(Path(\"saved_models\", \"LR3_model_DATA02\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [],
   "source": [
    "lr_schedule.initial_learning_rate = 0.0001"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"sequential_18\"\n",
      "_________________________________________________________________\n",
      " Layer (type)                Output Shape              Param #   \n",
      "=================================================================\n",
      " normalization (Normalizati  (None, 6)                 13        \n",
      " on)                                                             \n",
      "                                                                 \n",
      " dense_42 (Dense)            (None, 16)                112       \n",
      "                                                                 \n",
      " dense_43 (Dense)            (None, 2)                 34        \n",
      "                                                                 \n",
      "=================================================================\n",
      "Total params: 159 (640.00 Byte)\n",
      "Trainable params: 146 (584.00 Byte)\n",
      "Non-trainable params: 13 (56.00 Byte)\n",
      "_________________________________________________________________\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Assets written to: saved_models\\LR4_model_DATA01\\assets\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Assets written to: saved_models\\LR4_model_DATA01\\assets\n"
     ]
    }
   ],
   "source": [
    "OPT = Adam(lr_schedule)\n",
    "\n",
    "LR4_model = Sequential()\n",
    "LR4_model.add(DATA01_normalisation_biotite_composition)\n",
    "LR4_model.add(Dense(16, activation=\"relu\"))\n",
    "LR4_model.add(Dense(2))\n",
    "\n",
    "LR4_model.compile(optimizer=OPT, loss=LOSS, metrics=METRICS)\n",
    "LR4_model.summary()\n",
    "\n",
    "history = LR4_model.fit(DATA01_biotite_composition_train, DATA01_pt_train_norm,\n",
    "                        batch_size=BATCH_SIZE, epochs=MAX_EPOCHS,\n",
    "                        validation_data=[DATA01_biotite_composition_val, DATA01_pt_val_norm],\n",
    "                        callbacks=CALLBACKS_LR4_01, verbose=False)\n",
    "\n",
    "LR4_model.save(Path(\"saved_models\", \"LR4_model_DATA01\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"sequential_19\"\n",
      "_________________________________________________________________\n",
      " Layer (type)                Output Shape              Param #   \n",
      "=================================================================\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " normalization_1 (Normaliza  (None, 14)                29        \n",
      " tion)                                                           \n",
      "                                                                 \n",
      " dense_44 (Dense)            (None, 16)                240       \n",
      "                                                                 \n",
      " dense_45 (Dense)            (None, 2)                 34        \n",
      "                                                                 \n",
      "=================================================================\n",
      "Total params: 303 (1.19 KB)\n",
      "Trainable params: 274 (1.07 KB)\n",
      "Non-trainable params: 29 (120.00 Byte)\n",
      "_________________________________________________________________\n",
      "INFO:tensorflow:Assets written to: saved_models\\LR4_model_DATA02\\assets\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Assets written to: saved_models\\LR4_model_DATA02\\assets\n"
     ]
    }
   ],
   "source": [
    "OPT = Adam(lr_schedule)\n",
    "\n",
    "LR4_model = Sequential()\n",
    "LR4_model.add(DATA02_normalisation_biotite_composition)\n",
    "LR4_model.add(Dense(16, activation=\"relu\"))\n",
    "LR4_model.add(Dense(2))\n",
    "\n",
    "LR4_model.compile(optimizer=OPT, loss=LOSS, metrics=METRICS)\n",
    "LR4_model.summary()\n",
    "\n",
    "history = LR4_model.fit(DATA02_biotite_composition_train, DATA02_pt_train_norm,\n",
    "                        batch_size=BATCH_SIZE, epochs=MAX_EPOCHS,\n",
    "                        validation_data=[DATA02_biotite_composition_val, DATA02_pt_val_norm],\n",
    "                        callbacks=CALLBACKS_LR4_02, verbose=False)\n",
    "\n",
    "LR4_model.save(Path(\"saved_models\", \"LR4_model_DATA02\"))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## **Test 03:** Regularisation\n",
    "\n",
    "Test if regularisation can improve performance if applied to models with the best training set performance determined in tests 01 and 02.\\\n",
    "- Model architecture: huge. 3 hidden layers with 64 neurons each.\n",
    "- Dataset: Bt composition + index minerals."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [],
   "source": [
    "# RESER learning rate to 0.001\n",
    "lr_schedule.initial_learning_rate = 0.001"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [],
   "source": [
    "CALLBACKS_noReg = [CSVLogger(\"HyperParamTest_noReg.log\", append=False), EarlyStopping(monitor=\"loss\", patience=50)]\n",
    "CALLBACKS_L2 = [CSVLogger(\"HyperParamTest_L2.log\", append=False), EarlyStopping(monitor=\"loss\", patience=50)]\n",
    "CALLBACKS_dropout = [CSVLogger(\"HyperParamTest_dropout.log\", append=False), EarlyStopping(monitor=\"loss\", patience=50)]\n",
    "CALLBACKS_BatchNorm = [CSVLogger(\"HyperParamTest_BatchNorm.log\", append=False), EarlyStopping(monitor=\"loss\", patience=50)]\n",
    "CALLBACKS_LayerNorm = [CSVLogger(\"HyperParamTest_LayerNorm.log\", append=False), EarlyStopping(monitor=\"loss\", patience=50)]\n",
    "CALLBACKS_L2_dropout = [CSVLogger(\"HyperParamTest_L2_dropout.log\", append=False), EarlyStopping(monitor=\"loss\", patience=50)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"sequential_20\"\n",
      "_________________________________________________________________\n",
      " Layer (type)                Output Shape              Param #   \n",
      "=================================================================\n",
      " normalization_1 (Normaliza  (None, 14)                29        \n",
      " tion)                                                           \n",
      "                                                                 \n",
      " dense_46 (Dense)            (None, 64)                960       \n",
      "                                                                 \n",
      " dense_47 (Dense)            (None, 64)                4160      \n",
      "                                                                 \n",
      " dense_48 (Dense)            (None, 64)                4160      \n",
      "                                                                 \n",
      " dense_49 (Dense)            (None, 2)                 130       \n",
      "                                                                 \n",
      "=================================================================\n",
      "Total params: 9439 (36.88 KB)\n",
      "Trainable params: 9410 (36.76 KB)\n",
      "Non-trainable params: 29 (120.00 Byte)\n",
      "_________________________________________________________________\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Assets written to: saved_models\\noReg_model\\assets\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Assets written to: saved_models\\noReg_model\\assets\n"
     ]
    }
   ],
   "source": [
    "OPT = Adam(lr_schedule)\n",
    "\n",
    "noReg_model = Sequential()\n",
    "noReg_model.add(DATA02_normalisation_biotite_composition)\n",
    "noReg_model.add(Dense(64, activation=\"relu\"))\n",
    "noReg_model.add(Dense(64, activation=\"relu\"))\n",
    "noReg_model.add(Dense(64, activation=\"relu\"))\n",
    "noReg_model.add(Dense(2))\n",
    "\n",
    "noReg_model.compile(optimizer=OPT, loss=LOSS, metrics=METRICS)\n",
    "noReg_model.summary()\n",
    "\n",
    "history = noReg_model.fit(DATA02_biotite_composition_train, DATA02_pt_train_norm,\n",
    "                          batch_size=BATCH_SIZE, epochs=MAX_EPOCHS,\n",
    "                          validation_data=[DATA02_biotite_composition_val, DATA02_pt_val_norm],\n",
    "                          callbacks=CALLBACKS_noReg, verbose=False)\n",
    "noReg_model.save(Path(\"saved_models\", \"noReg_model\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"sequential_21\"\n",
      "_________________________________________________________________\n",
      " Layer (type)                Output Shape              Param #   \n",
      "=================================================================\n",
      " normalization_1 (Normaliza  (None, 14)                29        \n",
      " tion)                                                           \n",
      "                                                                 \n",
      " dense_50 (Dense)            (None, 64)                960       \n",
      "                                                                 \n",
      " dense_51 (Dense)            (None, 64)                4160      \n",
      "                                                                 \n",
      " dense_52 (Dense)            (None, 64)                4160      \n",
      "                                                                 \n",
      " dense_53 (Dense)            (None, 2)                 130       \n",
      "                                                                 \n",
      "=================================================================\n",
      "Total params: 9439 (36.88 KB)\n",
      "Trainable params: 9410 (36.76 KB)\n",
      "Non-trainable params: 29 (120.00 Byte)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "_________________________________________________________________\n",
      "INFO:tensorflow:Assets written to: saved_models\\L2_model\\assets\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Assets written to: saved_models\\L2_model\\assets\n"
     ]
    }
   ],
   "source": [
    "OPT = Adam(lr_schedule)\n",
    "\n",
    "L2_model = Sequential()\n",
    "L2_model.add(DATA02_normalisation_biotite_composition)\n",
    "L2_model.add(Dense(64, activation=\"relu\", kernel_regularizer=\"l2\"))\n",
    "L2_model.add(Dense(64, activation=\"relu\", kernel_regularizer=\"l2\"))\n",
    "L2_model.add(Dense(64, activation=\"relu\", kernel_regularizer=\"l2\"))\n",
    "L2_model.add(Dense(2))\n",
    "\n",
    "L2_model.compile(optimizer=OPT, loss=LOSS, metrics=METRICS)\n",
    "L2_model.summary()\n",
    "\n",
    "history = L2_model.fit(DATA02_biotite_composition_train, DATA02_pt_train_norm,\n",
    "                          batch_size=BATCH_SIZE, epochs=MAX_EPOCHS,\n",
    "                          validation_data=[DATA02_biotite_composition_val, DATA02_pt_val_norm],\n",
    "                          callbacks=CALLBACKS_L2, verbose=False)\n",
    "\n",
    "L2_model.save(Path(\"saved_models\", \"L2_model\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"sequential_22\"\n",
      "_________________________________________________________________\n",
      " Layer (type)                Output Shape              Param #   \n",
      "=================================================================\n",
      " normalization_1 (Normaliza  (None, 14)                29        \n",
      " tion)                                                           \n",
      "                                                                 \n",
      " dense_54 (Dense)            (None, 64)                960       \n",
      "                                                                 \n",
      " dropout (Dropout)           (None, 64)                0         \n",
      "                                                                 \n",
      " dense_55 (Dense)            (None, 64)                4160      \n",
      "                                                                 \n",
      " dropout_1 (Dropout)         (None, 64)                0         \n",
      "                                                                 \n",
      " dense_56 (Dense)            (None, 64)                4160      \n",
      "                                                                 \n",
      " dropout_2 (Dropout)         (None, 64)                0         \n",
      "                                                                 \n",
      " dense_57 (Dense)            (None, 2)                 130       \n",
      "                                                                 \n",
      "=================================================================\n",
      "Total params: 9439 (36.88 KB)\n",
      "Trainable params: 9410 (36.76 KB)\n",
      "Non-trainable params: 29 (120.00 Byte)\n",
      "_________________________________________________________________\n",
      "INFO:tensorflow:Assets written to: saved_models\\dropout_model\\assets\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Assets written to: saved_models\\dropout_model\\assets\n"
     ]
    }
   ],
   "source": [
    "OPT = Adam(lr_schedule)\n",
    "\n",
    "dropout_model = Sequential()\n",
    "dropout_model.add(DATA02_normalisation_biotite_composition)\n",
    "dropout_model.add(Dense(64, activation=\"relu\"))\n",
    "dropout_model.add(Dropout(0.2))\n",
    "dropout_model.add(Dense(64, activation=\"relu\"))\n",
    "dropout_model.add(Dropout(0.2))\n",
    "dropout_model.add(Dense(64, activation=\"relu\"))\n",
    "dropout_model.add(Dropout(0.2))\n",
    "dropout_model.add(Dense(2))\n",
    "\n",
    "dropout_model.compile(optimizer=OPT, loss=LOSS, metrics=METRICS)\n",
    "dropout_model.summary()\n",
    "\n",
    "history = dropout_model.fit(DATA02_biotite_composition_train, DATA02_pt_train_norm,\n",
    "                         batch_size=BATCH_SIZE, epochs=MAX_EPOCHS,\n",
    "                         validation_data=[DATA02_biotite_composition_val, DATA02_pt_val_norm],\n",
    "                         callbacks=CALLBACKS_dropout, verbose=False)\n",
    "dropout_model.save(Path(\"saved_models\", \"dropout_model\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"sequential_23\"\n",
      "_________________________________________________________________\n",
      " Layer (type)                Output Shape              Param #   \n",
      "=================================================================\n",
      " normalization_1 (Normaliza  (None, 14)                29        \n",
      " tion)                                                           \n",
      "                                                                 \n",
      " dense_58 (Dense)            (None, 64)                960       \n",
      "                                                                 \n",
      " batch_normalization (Batch  (None, 64)                256       \n",
      " Normalization)                                                  \n",
      "                                                                 \n",
      " dense_59 (Dense)            (None, 64)                4160      \n",
      "                                                                 \n",
      " batch_normalization_1 (Bat  (None, 64)                256       \n",
      " chNormalization)                                                \n",
      "                                                                 \n",
      " dense_60 (Dense)            (None, 64)                4160      \n",
      "                                                                 \n",
      " batch_normalization_2 (Bat  (None, 64)                256       \n",
      " chNormalization)                                                \n",
      "                                                                 \n",
      " dense_61 (Dense)            (None, 2)                 130       \n",
      "                                                                 \n",
      "=================================================================\n",
      "Total params: 10207 (39.88 KB)\n",
      "Trainable params: 9794 (38.26 KB)\n",
      "Non-trainable params: 413 (1.62 KB)\n",
      "_________________________________________________________________\n",
      "INFO:tensorflow:Assets written to: saved_models\\BatchNorm_model\\assets\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Assets written to: saved_models\\BatchNorm_model\\assets\n"
     ]
    }
   ],
   "source": [
    "OPT = Adam(lr_schedule)\n",
    "\n",
    "BatchNorm_model = Sequential()\n",
    "BatchNorm_model.add(DATA02_normalisation_biotite_composition)\n",
    "BatchNorm_model.add(Dense(64, activation=\"relu\"))\n",
    "BatchNorm_model.add(BatchNormalization())\n",
    "BatchNorm_model.add(Dense(64, activation=\"relu\"))\n",
    "BatchNorm_model.add(BatchNormalization())\n",
    "BatchNorm_model.add(Dense(64, activation=\"relu\"))\n",
    "BatchNorm_model.add(BatchNormalization())\n",
    "BatchNorm_model.add(Dense(2))\n",
    "\n",
    "BatchNorm_model.compile(optimizer=OPT, loss=LOSS, metrics=METRICS)\n",
    "BatchNorm_model.summary()\n",
    "\n",
    "history = BatchNorm_model.fit(DATA02_biotite_composition_train, DATA02_pt_train_norm,\n",
    "                              batch_size=BATCH_SIZE, epochs=MAX_EPOCHS,\n",
    "                              validation_data=[DATA02_biotite_composition_val, DATA02_pt_val_norm],\n",
    "                              callbacks=CALLBACKS_BatchNorm, verbose=False)\n",
    "BatchNorm_model.save(Path(\"saved_models\", \"BatchNorm_model\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:From c:\\Users\\ThinkPad\\miniconda3\\envs\\masterproject\\Lib\\site-packages\\keras\\src\\layers\\normalization\\layer_normalization.py:328: The name tf.nn.fused_batch_norm is deprecated. Please use tf.compat.v1.nn.fused_batch_norm instead.\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:From c:\\Users\\ThinkPad\\miniconda3\\envs\\masterproject\\Lib\\site-packages\\keras\\src\\layers\\normalization\\layer_normalization.py:328: The name tf.nn.fused_batch_norm is deprecated. Please use tf.compat.v1.nn.fused_batch_norm instead.\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"sequential_24\"\n",
      "_________________________________________________________________\n",
      " Layer (type)                Output Shape              Param #   \n",
      "=================================================================\n",
      " normalization_1 (Normaliza  (None, 14)                29        \n",
      " tion)                                                           \n",
      "                                                                 \n",
      " dense_62 (Dense)            (None, 64)                960       \n",
      "                                                                 \n",
      " layer_normalization (Layer  (None, 64)                128       \n",
      " Normalization)                                                  \n",
      "                                                                 \n",
      " dense_63 (Dense)            (None, 64)                4160      \n",
      "                                                                 \n",
      " layer_normalization_1 (Lay  (None, 64)                128       \n",
      " erNormalization)                                                \n",
      "                                                                 \n",
      " dense_64 (Dense)            (None, 64)                4160      \n",
      "                                                                 \n",
      " layer_normalization_2 (Lay  (None, 64)                128       \n",
      " erNormalization)                                                \n",
      "                                                                 \n",
      " dense_65 (Dense)            (None, 2)                 130       \n",
      "                                                                 \n",
      "=================================================================\n",
      "Total params: 9823 (38.38 KB)\n",
      "Trainable params: 9794 (38.26 KB)\n",
      "Non-trainable params: 29 (120.00 Byte)\n",
      "_________________________________________________________________\n",
      "INFO:tensorflow:Assets written to: saved_models\\LayerNorm_model\\assets\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Assets written to: saved_models\\LayerNorm_model\\assets\n"
     ]
    }
   ],
   "source": [
    "OPT = Adam(lr_schedule)\n",
    "\n",
    "LayerNorm_model = Sequential()\n",
    "LayerNorm_model.add(DATA02_normalisation_biotite_composition)\n",
    "LayerNorm_model.add(Dense(64, activation=\"relu\"))\n",
    "LayerNorm_model.add(LayerNormalization())\n",
    "LayerNorm_model.add(Dense(64, activation=\"relu\"))\n",
    "LayerNorm_model.add(LayerNormalization())\n",
    "LayerNorm_model.add(Dense(64, activation=\"relu\"))\n",
    "LayerNorm_model.add(LayerNormalization())\n",
    "LayerNorm_model.add(Dense(2))\n",
    "\n",
    "LayerNorm_model.compile(optimizer=OPT, loss=LOSS, metrics=METRICS)\n",
    "LayerNorm_model.summary()\n",
    "\n",
    "history = LayerNorm_model.fit(DATA02_biotite_composition_train, DATA02_pt_train_norm,\n",
    "                              batch_size=BATCH_SIZE, epochs=MAX_EPOCHS,\n",
    "                              validation_data=[DATA02_biotite_composition_val, DATA02_pt_val_norm],\n",
    "                              callbacks=CALLBACKS_LayerNorm, verbose=False)\n",
    "LayerNorm_model.save(Path(\"saved_models\", \"LayerNorm_model\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"sequential_25\"\n",
      "_________________________________________________________________\n",
      " Layer (type)                Output Shape              Param #   \n",
      "=================================================================\n",
      " normalization_1 (Normaliza  (None, 14)                29        \n",
      " tion)                                                           \n",
      "                                                                 \n",
      " dense_66 (Dense)            (None, 64)                960       \n",
      "                                                                 \n",
      " dropout_3 (Dropout)         (None, 64)                0         \n",
      "                                                                 \n",
      " dense_67 (Dense)            (None, 64)                4160      \n",
      "                                                                 \n",
      " dropout_4 (Dropout)         (None, 64)                0         \n",
      "                                                                 \n",
      " dense_68 (Dense)            (None, 64)                4160      \n",
      "                                                                 \n",
      " dropout_5 (Dropout)         (None, 64)                0         \n",
      "                                                                 \n",
      " dense_69 (Dense)            (None, 2)                 130       \n",
      "                                                                 \n",
      "=================================================================\n",
      "Total params: 9439 (36.88 KB)\n",
      "Trainable params: 9410 (36.76 KB)\n",
      "Non-trainable params: 29 (120.00 Byte)\n",
      "_________________________________________________________________\n",
      "INFO:tensorflow:Assets written to: saved_models\\L2_dropout_model\\assets\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Assets written to: saved_models\\L2_dropout_model\\assets\n"
     ]
    }
   ],
   "source": [
    "OPT = Adam(lr_schedule)\n",
    "\n",
    "l2_dropout_model = Sequential()\n",
    "l2_dropout_model.add(DATA02_normalisation_biotite_composition)\n",
    "l2_dropout_model.add(Dense(64, activation=\"relu\", kernel_regularizer=\"l2\"))\n",
    "l2_dropout_model.add(Dropout(0.2))\n",
    "l2_dropout_model.add(Dense(64, activation=\"relu\", kernel_regularizer=\"l2\"))\n",
    "l2_dropout_model.add(Dropout(0.2))\n",
    "l2_dropout_model.add(Dense(64, activation=\"relu\", kernel_regularizer=\"l2\"))\n",
    "l2_dropout_model.add(Dropout(0.2))\n",
    "l2_dropout_model.add(Dense(2))\n",
    "\n",
    "l2_dropout_model.compile(optimizer=OPT, loss=LOSS, metrics=METRICS)\n",
    "l2_dropout_model.summary()\n",
    "\n",
    "history = l2_dropout_model.fit(DATA02_biotite_composition_train, DATA02_pt_train_norm,\n",
    "                              batch_size=BATCH_SIZE, epochs=MAX_EPOCHS,\n",
    "                              validation_data=[DATA02_biotite_composition_val, DATA02_pt_val_norm],\n",
    "                              callbacks=CALLBACKS_L2_dropout, verbose=False)\n",
    "l2_dropout_model.save(Path(\"saved_models\", \"L2_dropout_model\"))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "master",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
