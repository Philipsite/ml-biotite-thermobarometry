{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    ":warning:**IMPORTANT NOTICE**:warning:\\\n",
    "*Since the initial parameterisation and gradient descent optimisation are stochastic processes, the training of a neural network is not fully reproducible.*\n",
    "\n",
    "*Therefore, it is not recommended to re-run this script as it will overwrite the original calibration of the neural network used in the work presented here.\n",
    "The purpose of this script is solely to document the training procedure and can be copied as a template to fit other new neural networks.*\n",
    "\n",
    "*To experiment with the models calibrated here, they can be loaded from the `saved_models` directory.*"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Systematic assesment of different hyperparameters to optimise model performance\n",
    "This notebook is used to find the optimal hyperparameters fot the NN-thermometer based on XMg and Ti.\\\n",
    "**Only a thermometer, no barometer.**\n",
    "\n",
    "Various hyperparameters are varied to asses their impact on the model performance in a semi-quantitative way.\\\n",
    "The hyperparameters that are varied are:\n",
    "- Number of hidden layers / neurons --> model capacity\n",
    "- Initial learning rate\n",
    "- Activation function\n",
    "- Effect of regularization\n",
    "\n",
    "### Model capacity\n",
    "\n",
    "These architectures are tested:\n",
    "- **very small**: 1 hidden layers with 8 neurons each\n",
    "- **small**: 1 hidden layers with 16 neurons each\n",
    "- **large**: 1 hidden layers with 32 neurons each\n",
    "- **small_2hl**: 2 hidden layers with 16 neurons each\n",
    "- **large_2hl**: 2 hidden layers with 32 neurons each\n",
    "- **huge**: 3 hidden layers with 64 neurons\n",
    "\n",
    "### Learning rate\n",
    "\n",
    "These learning rates are tested:\n",
    "- 0.01\n",
    "- 0.001\n",
    "- 0.0005\n",
    "- 0.0001\n",
    "\n",
    "### Activation function\n",
    "*Decided against testing!*"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\ThinkPad\\AppData\\Local\\Temp\\ipykernel_3108\\3182093084.py:2: DeprecationWarning: \n",
      "Pyarrow will become a required dependency of pandas in the next major release of pandas (pandas 3.0),\n",
      "(to allow more performant data types, such as the Arrow string type, and better interoperability with other libraries)\n",
      "but was not found to be installed on your system.\n",
      "If this would cause problems for you,\n",
      "please provide us feedback at https://github.com/pandas-dev/pandas/issues/54466\n",
      "        \n",
      "  import pandas as pd\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:From c:\\Users\\ThinkPad\\miniconda3\\envs\\masterproject\\Lib\\site-packages\\keras\\src\\losses.py:2976: The name tf.losses.sparse_softmax_cross_entropy is deprecated. Please use tf.compat.v1.losses.sparse_softmax_cross_entropy instead.\n",
      "\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "import tensorflow as tf\n",
    "\n",
    "from pathlib import Path\n",
    "from keras.models import Sequential\n",
    "from keras.layers import Dense, Normalization, BatchNormalization, LayerNormalization, Dropout\n",
    "from keras.losses import MeanSquaredError\n",
    "from keras.optimizers import Adam, schedules\n",
    "from keras.metrics import MeanAbsoluteError, RootMeanSquaredError\n",
    "from keras.callbacks import CSVLogger, EarlyStopping\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "from ml_tb.normalisation import MinMaxScaler\n",
    "from ml_tb.metrics import RMSE_denormalised_temperature_only"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Import data, pre-procesing and train/test split\n",
    "\n",
    "Load the two datasets and split them into training and validation sets.\n",
    "\n",
    "Validation set is **20%** of the training set. Approx. 200 samples."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "VALIDATION_FRACTION = 0.2"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "For the scaling of the P-T data, a MinMaxScaler is used.\n",
    "This scaler is defined globally and used for all datasets."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:From c:\\Users\\ThinkPad\\miniconda3\\envs\\masterproject\\Lib\\site-packages\\keras\\src\\backend.py:873: The name tf.get_default_graph is deprecated. Please use tf.compat.v1.get_default_graph instead.\n",
      "\n"
     ]
    }
   ],
   "source": [
    "scaling_pt = MinMaxScaler(min=400, max=900, axis=0)\n",
    "inv_scaling_pt = MinMaxScaler(min=400, max=900, axis=0, invert=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### **Dataset: Ti-XMg**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "NaN values in biotite composition:  False\n",
      "NaN values in PT:  False\n"
     ]
    }
   ],
   "source": [
    "# load excel file\n",
    "data = pd.read_excel(Path(\"Metapelite-Database_Bt_CLEAN_2024-02-03.xlsx\"))\n",
    "\n",
    "biotite_composition = np.zeros(shape=(len(data), 2))\n",
    "biotite_composition[:, 0] = data[\"Bt-Ti\"]\n",
    "biotite_composition[:, 1] = data[\"Bt-XMg\"]\n",
    "\n",
    "\n",
    "temperature = np.zeros(shape=(len(data), 1))\n",
    "temperature = data[\"Temperature random ordered after Ti-in-Bt\"]\n",
    "\n",
    "# check for NaN values (should be already filtered out)\n",
    "print(\"NaN values in biotite composition: \", np.isnan(biotite_composition).any())\n",
    "print(\"NaN values in PT: \", np.isnan(temperature).any())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[0.12126549 0.49694127]]\n",
      "[[0.04824786 0.10380844]]\n",
      "After normalisation, the minimal value of P and T is:  0.00066033937\n",
      "After normalisation, the maximal value of P and T is:  0.8988128\n"
     ]
    }
   ],
   "source": [
    "# test train split\n",
    "biotite_composition_train, biotite_composition_val, pt_train, pt_val = train_test_split(biotite_composition, temperature, test_size=VALIDATION_FRACTION, shuffle=True)\n",
    "\n",
    "normalisation_biotite_composition = Normalization(axis=-1)\n",
    "normalisation_biotite_composition.adapt(biotite_composition_train)\n",
    "\n",
    "print(normalisation_biotite_composition.mean.numpy())\n",
    "print(np.sqrt(normalisation_biotite_composition.variance.numpy()))\n",
    "\n",
    "pt_train_norm = scaling_pt(pt_train)\n",
    "pt_val_norm = scaling_pt(pt_val)\n",
    "\n",
    "print(\"After normalisation, the minimal value of P and T is: \", pt_train_norm.numpy().min(axis=0))\n",
    "print(\"After normalisation, the maximal value of P and T is: \", pt_train_norm.numpy().max(axis=0))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Set-up global training parameters\n",
    "\n",
    "Define a function to calculate RMSE for pressure and temperature for unscaled values to have an interpretable metric.\n",
    "\n",
    "All models are trained for a maximum of 5000 epochs.\\\n",
    "Early stopping is used with a patience (no improvement of val loss) of 50 epochs.\\\n",
    "Inverse time learning rate decay is used for all models."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "def RMSE_T(y_true, y_pred):\n",
    "    return RMSE_denormalised_temperature_only(y_true, y_pred, inv_scaling_pt)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "BATCH_SIZE = 50\n",
    "STEPS_PER_EPOCH = len(biotite_composition_train) // BATCH_SIZE\n",
    "MAX_EPOCHS = 5000\n",
    "\n",
    "lr_schedule = schedules.InverseTimeDecay(0.001, decay_steps=STEPS_PER_EPOCH*1000, decay_rate=1, staircase=False)\n",
    "\n",
    "LOSS = MeanSquaredError()\n",
    "METRICS = [MeanAbsoluteError(), RootMeanSquaredError(), RMSE_T]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## **Test 01:** Model capacity"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- **very small**: 1 hidden layers with 4 neurons each\n",
    "- **small**: 1 hidden layers with 16 neurons each\n",
    "- **large**: 1 hidden layers with 32 neurons each\n",
    "- **small_2hl**: 2 hidden layers with 16 neurons each\n",
    "- **large_2hl**: 2 hidden layers with 32 neurons each\n",
    "- **huge**: 3 hidden layers with 64 neurons"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "CALLBACKS_VERYSMALL = [CSVLogger(\"HyperParamTest_verysmall_TiXMg.log\", append=False), EarlyStopping(monitor=\"loss\", patience=50)]\n",
    "CALLBACKS_SMALL = [CSVLogger(\"HyperParamTest_small_TiXMg.log\", append=False), EarlyStopping(monitor=\"loss\", patience=50)]\n",
    "CALLBACKS_LARGE = [CSVLogger(\"HyperParamTest_large_TiXMg.log\", append=False), EarlyStopping(monitor=\"loss\", patience=50)]\n",
    "CALLBACKS_SMALL_2HL = [CSVLogger(\"HyperParamTest_small_2HL_TiXMg.log\", append=False), EarlyStopping(monitor=\"loss\", patience=50)]\n",
    "CALLBACKS_LARGE_2HL = [CSVLogger(\"HyperParamTest_large_2HL_TiXMg.log\", append=False), EarlyStopping(monitor=\"loss\", patience=50)]\n",
    "CALLBACKS_HUGE = [CSVLogger(\"HyperParamTest_huge_TiXMg.log\", append=False), EarlyStopping(monitor=\"loss\", patience=50)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"sequential\"\n",
      "_________________________________________________________________\n",
      " Layer (type)                Output Shape              Param #   \n",
      "=================================================================\n",
      " normalization (Normalizati  (None, 2)                 5         \n",
      " on)                                                             \n",
      "                                                                 \n",
      " dense (Dense)               (None, 4)                 12        \n",
      "                                                                 \n",
      " dense_1 (Dense)             (None, 1)                 5         \n",
      "                                                                 \n",
      "=================================================================\n",
      "Total params: 22 (92.00 Byte)\n",
      "Trainable params: 17 (68.00 Byte)\n",
      "Non-trainable params: 5 (24.00 Byte)\n",
      "_________________________________________________________________\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:From c:\\Users\\ThinkPad\\miniconda3\\envs\\masterproject\\Lib\\site-packages\\keras\\src\\utils\\tf_utils.py:492: The name tf.ragged.RaggedTensorValue is deprecated. Please use tf.compat.v1.ragged.RaggedTensorValue instead.\n",
      "\n",
      "WARNING:tensorflow:From c:\\Users\\ThinkPad\\miniconda3\\envs\\masterproject\\Lib\\site-packages\\keras\\src\\engine\\base_layer_utils.py:384: The name tf.executing_eagerly_outside_functions is deprecated. Please use tf.compat.v1.executing_eagerly_outside_functions instead.\n",
      "\n",
      "INFO:tensorflow:Assets written to: saved_models\\verysmall_model_TiXMg\\assets\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Assets written to: saved_models\\verysmall_model_TiXMg\\assets\n"
     ]
    }
   ],
   "source": [
    "OPT = Adam(lr_schedule)\n",
    "\n",
    "verysmall_model = Sequential()\n",
    "verysmall_model.add(normalisation_biotite_composition)\n",
    "verysmall_model.add(Dense(4, activation=\"relu\"))\n",
    "verysmall_model.add(Dense(1))\n",
    "\n",
    "verysmall_model.compile(optimizer=OPT, loss=LOSS, metrics=METRICS)\n",
    "verysmall_model.summary()\n",
    "\n",
    "history = verysmall_model.fit(biotite_composition_train, pt_train_norm,\n",
    "                          batch_size=BATCH_SIZE, epochs=MAX_EPOCHS,\n",
    "                          validation_data=[biotite_composition_val, pt_val_norm],\n",
    "                          callbacks=CALLBACKS_VERYSMALL, verbose=False)\n",
    "verysmall_model.save(Path(\"saved_models\", \"verysmall_model_TiXMg\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"sequential_1\"\n",
      "_________________________________________________________________\n",
      " Layer (type)                Output Shape              Param #   \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "=================================================================\n",
      " normalization (Normalizati  (None, 2)                 5         \n",
      " on)                                                             \n",
      "                                                                 \n",
      " dense_2 (Dense)             (None, 16)                48        \n",
      "                                                                 \n",
      " dense_3 (Dense)             (None, 1)                 17        \n",
      "                                                                 \n",
      "=================================================================\n",
      "Total params: 70 (284.00 Byte)\n",
      "Trainable params: 65 (260.00 Byte)\n",
      "Non-trainable params: 5 (24.00 Byte)\n",
      "_________________________________________________________________\n",
      "INFO:tensorflow:Assets written to: saved_models\\small_model_TiXMg\\assets\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Assets written to: saved_models\\small_model_TiXMg\\assets\n"
     ]
    }
   ],
   "source": [
    "OPT = Adam(lr_schedule)\n",
    "\n",
    "small_model = Sequential()\n",
    "small_model.add(normalisation_biotite_composition)\n",
    "small_model.add(Dense(16, activation=\"relu\"))\n",
    "small_model.add(Dense(1))\n",
    "\n",
    "small_model.compile(optimizer=OPT, loss=LOSS, metrics=METRICS)\n",
    "small_model.summary()\n",
    "\n",
    "history = small_model.fit(biotite_composition_train, pt_train_norm,\n",
    "                          batch_size=BATCH_SIZE, epochs=MAX_EPOCHS,\n",
    "                          validation_data=[biotite_composition_val, pt_val_norm],\n",
    "                          callbacks=CALLBACKS_SMALL, verbose=False)\n",
    "small_model.save(Path(\"saved_models\", \"small_model_TiXMg\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"sequential_2\"\n",
      "_________________________________________________________________\n",
      " Layer (type)                Output Shape              Param #   \n",
      "=================================================================\n",
      " normalization (Normalizati  (None, 2)                 5         \n",
      " on)                                                             \n",
      "                                                                 \n",
      " dense_4 (Dense)             (None, 32)                96        \n",
      "                                                                 \n",
      " dense_5 (Dense)             (None, 1)                 33        \n",
      "                                                                 \n",
      "=================================================================\n",
      "Total params: 134 (540.00 Byte)\n",
      "Trainable params: 129 (516.00 Byte)\n",
      "Non-trainable params: 5 (24.00 Byte)\n",
      "_________________________________________________________________\n",
      "INFO:tensorflow:Assets written to: saved_models\\large_model_TiXMg\\assets\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Assets written to: saved_models\\large_model_TiXMg\\assets\n"
     ]
    }
   ],
   "source": [
    "OPT = Adam(lr_schedule)\n",
    "\n",
    "large_model = Sequential()\n",
    "large_model.add(normalisation_biotite_composition)\n",
    "large_model.add(Dense(32, activation=\"relu\"))\n",
    "large_model.add(Dense(1))\n",
    "\n",
    "large_model.compile(optimizer=OPT, loss=LOSS, metrics=METRICS)\n",
    "large_model.summary()\n",
    "\n",
    "history = large_model.fit(biotite_composition_train, pt_train_norm,\n",
    "                          batch_size=BATCH_SIZE, epochs=MAX_EPOCHS,\n",
    "                          validation_data=[biotite_composition_val, pt_val_norm],\n",
    "                          callbacks=CALLBACKS_LARGE, verbose=False)\n",
    "large_model.save(Path(\"saved_models\", \"large_model_TiXMg\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"sequential_3\"\n",
      "_________________________________________________________________\n",
      " Layer (type)                Output Shape              Param #   \n",
      "=================================================================\n",
      " normalization (Normalizati  (None, 2)                 5         \n",
      " on)                                                             \n",
      "                                                                 \n",
      " dense_6 (Dense)             (None, 16)                48        \n",
      "                                                                 \n",
      " dense_7 (Dense)             (None, 16)                272       \n",
      "                                                                 \n",
      " dense_8 (Dense)             (None, 1)                 17        \n",
      "                                                                 \n",
      "=================================================================\n",
      "Total params: 342 (1.34 KB)\n",
      "Trainable params: 337 (1.32 KB)\n",
      "Non-trainable params: 5 (24.00 Byte)\n",
      "_________________________________________________________________\n",
      "INFO:tensorflow:Assets written to: saved_models\\small_2hl_model_TiXMg\\assets\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Assets written to: saved_models\\small_2hl_model_TiXMg\\assets\n"
     ]
    }
   ],
   "source": [
    "OPT = Adam(lr_schedule)\n",
    "\n",
    "small_2hl_model = Sequential()\n",
    "small_2hl_model.add(normalisation_biotite_composition)\n",
    "small_2hl_model.add(Dense(16, activation=\"relu\"))\n",
    "small_2hl_model.add(Dense(16, activation=\"relu\"))\n",
    "small_2hl_model.add(Dense(1))\n",
    "\n",
    "small_2hl_model.compile(optimizer=OPT, loss=LOSS, metrics=METRICS)\n",
    "small_2hl_model.summary()\n",
    "\n",
    "history = small_2hl_model.fit(biotite_composition_train, pt_train_norm,\n",
    "                          batch_size=BATCH_SIZE, epochs=MAX_EPOCHS,\n",
    "                          validation_data=[biotite_composition_val, pt_val_norm],\n",
    "                          callbacks=CALLBACKS_SMALL_2HL, verbose=False)\n",
    "small_2hl_model.save(Path(\"saved_models\", \"small_2hl_model_TiXMg\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"sequential_4\"\n",
      "_________________________________________________________________\n",
      " Layer (type)                Output Shape              Param #   \n",
      "=================================================================\n",
      " normalization (Normalizati  (None, 2)                 5         \n",
      " on)                                                             \n",
      "                                                                 \n",
      " dense_9 (Dense)             (None, 32)                96        \n",
      "                                                                 \n",
      " dense_10 (Dense)            (None, 32)                1056      \n",
      "                                                                 \n",
      " dense_11 (Dense)            (None, 1)                 33        \n",
      "                                                                 \n",
      "=================================================================\n",
      "Total params: 1190 (4.65 KB)\n",
      "Trainable params: 1185 (4.63 KB)\n",
      "Non-trainable params: 5 (24.00 Byte)\n",
      "_________________________________________________________________\n",
      "INFO:tensorflow:Assets written to: saved_models\\large_2hl_model_TiXMg\\assets\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Assets written to: saved_models\\large_2hl_model_TiXMg\\assets\n"
     ]
    }
   ],
   "source": [
    "OPT = Adam(lr_schedule)\n",
    "\n",
    "large_2hl_model = Sequential()\n",
    "large_2hl_model.add(normalisation_biotite_composition)\n",
    "large_2hl_model.add(Dense(32, activation=\"relu\"))\n",
    "large_2hl_model.add(Dense(32, activation=\"relu\"))\n",
    "large_2hl_model.add(Dense(1))\n",
    "\n",
    "large_2hl_model.compile(optimizer=OPT, loss=LOSS, metrics=METRICS)\n",
    "large_2hl_model.summary()\n",
    "\n",
    "history = large_2hl_model.fit(biotite_composition_train, pt_train_norm,\n",
    "                          batch_size=BATCH_SIZE, epochs=MAX_EPOCHS,\n",
    "                          validation_data=[biotite_composition_val, pt_val_norm],\n",
    "                          callbacks=CALLBACKS_LARGE_2HL, verbose=False)\n",
    "large_2hl_model.save(Path(\"saved_models\", \"large_2hl_model_TiXMg\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"sequential_5\"\n",
      "_________________________________________________________________\n",
      " Layer (type)                Output Shape              Param #   \n",
      "=================================================================\n",
      " normalization (Normalizati  (None, 2)                 5         \n",
      " on)                                                             \n",
      "                                                                 \n",
      " dense_12 (Dense)            (None, 64)                192       \n",
      "                                                                 \n",
      " dense_13 (Dense)            (None, 64)                4160      \n",
      "                                                                 \n",
      " dense_14 (Dense)            (None, 64)                4160      \n",
      "                                                                 \n",
      " dense_15 (Dense)            (None, 1)                 65        \n",
      "                                                                 \n",
      "=================================================================\n",
      "Total params: 8582 (33.53 KB)\n",
      "Trainable params: 8577 (33.50 KB)\n",
      "Non-trainable params: 5 (24.00 Byte)\n",
      "_________________________________________________________________\n",
      "INFO:tensorflow:Assets written to: saved_models\\huge_model_TiXMg\\assets\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Assets written to: saved_models\\huge_model_TiXMg\\assets\n"
     ]
    }
   ],
   "source": [
    "OPT = Adam(lr_schedule)\n",
    "\n",
    "huge_model = Sequential()\n",
    "huge_model.add(normalisation_biotite_composition)\n",
    "huge_model.add(Dense(64, activation=\"relu\"))\n",
    "huge_model.add(Dense(64, activation=\"relu\"))\n",
    "huge_model.add(Dense(64, activation=\"relu\"))\n",
    "huge_model.add(Dense(1))\n",
    "\n",
    "huge_model.compile(optimizer=OPT, loss=LOSS, metrics=METRICS)\n",
    "huge_model.summary()\n",
    "\n",
    "history = huge_model.fit(biotite_composition_train, pt_train_norm,\n",
    "                          batch_size=BATCH_SIZE, epochs=MAX_EPOCHS,\n",
    "                          validation_data=[biotite_composition_val, pt_val_norm],\n",
    "                          callbacks=CALLBACKS_HUGE, verbose=False)\n",
    "huge_model.save(Path(\"saved_models\", \"huge_model_TiXMg\"))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## **Test 02:** Learning rate\n",
    "\n",
    "All tests are performed with the optimal model architecture from Test 01. --> *small*\n",
    "\n",
    "These learning rates are tested:\n",
    "- 0.01\n",
    "- 0.001\n",
    "- 0.0005\n",
    "- 0.0001"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "CALLBACKS_LR1 = [CSVLogger(\"HyperParamTest_LR1_TiXMg.log\", append=False), EarlyStopping(monitor=\"loss\", patience=50)]\n",
    "CALLBACKS_LR2 = [CSVLogger(\"HyperParamTest_LR2_TiXMg.log\", append=False), EarlyStopping(monitor=\"loss\", patience=50)]\n",
    "CALLBACKS_LR3 = [CSVLogger(\"HyperParamTest_LR3_TiXMg.log\", append=False), EarlyStopping(monitor=\"loss\", patience=50)]\n",
    "CALLBACKS_LR4 = [CSVLogger(\"HyperParamTest_LR4_TiXMg.log\", append=False), EarlyStopping(monitor=\"loss\", patience=50)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "lr_schedule.initial_learning_rate = 0.01"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"sequential_6\"\n",
      "_________________________________________________________________\n",
      " Layer (type)                Output Shape              Param #   \n",
      "=================================================================\n",
      " normalization (Normalizati  (None, 2)                 5         \n",
      " on)                                                             \n",
      "                                                                 \n",
      " dense_16 (Dense)            (None, 16)                48        \n",
      "                                                                 \n",
      " dense_17 (Dense)            (None, 1)                 17        \n",
      "                                                                 \n",
      "=================================================================\n",
      "Total params: 70 (284.00 Byte)\n",
      "Trainable params: 65 (260.00 Byte)\n",
      "Non-trainable params: 5 (24.00 Byte)\n",
      "_________________________________________________________________\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Assets written to: saved_models\\LR1_model_TiXMg\\assets\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Assets written to: saved_models\\LR1_model_TiXMg\\assets\n"
     ]
    }
   ],
   "source": [
    "OPT = Adam(lr_schedule)\n",
    "\n",
    "LR1_model = Sequential()\n",
    "LR1_model.add(normalisation_biotite_composition)\n",
    "LR1_model.add(Dense(16, activation=\"relu\"))\n",
    "LR1_model.add(Dense(1))\n",
    "\n",
    "LR1_model.compile(optimizer=OPT, loss=LOSS, metrics=METRICS)\n",
    "LR1_model.summary()\n",
    "\n",
    "history = LR1_model.fit(biotite_composition_train, pt_train_norm,\n",
    "                          batch_size=BATCH_SIZE, epochs=MAX_EPOCHS,\n",
    "                          validation_data=[biotite_composition_val, pt_val_norm],\n",
    "                          callbacks=CALLBACKS_LR1, verbose=False)\n",
    "LR1_model.save(Path(\"saved_models\", \"LR1_model_TiXMg\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "lr_schedule.initial_learning_rate = 0.001"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"sequential_7\"\n",
      "_________________________________________________________________\n",
      " Layer (type)                Output Shape              Param #   \n",
      "=================================================================\n",
      " normalization (Normalizati  (None, 2)                 5         \n",
      " on)                                                             \n",
      "                                                                 \n",
      " dense_18 (Dense)            (None, 16)                48        \n",
      "                                                                 \n",
      " dense_19 (Dense)            (None, 1)                 17        \n",
      "                                                                 \n",
      "=================================================================\n",
      "Total params: 70 (284.00 Byte)\n",
      "Trainable params: 65 (260.00 Byte)\n",
      "Non-trainable params: 5 (24.00 Byte)\n",
      "_________________________________________________________________\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Assets written to: saved_models\\LR2_model_TiXMg\\assets\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Assets written to: saved_models\\LR2_model_TiXMg\\assets\n"
     ]
    }
   ],
   "source": [
    "OPT = Adam(lr_schedule)\n",
    "\n",
    "LR2_model = Sequential()\n",
    "LR2_model.add(normalisation_biotite_composition)\n",
    "LR2_model.add(Dense(16, activation=\"relu\"))\n",
    "LR2_model.add(Dense(1))\n",
    "\n",
    "LR2_model.compile(optimizer=OPT, loss=LOSS, metrics=METRICS)\n",
    "LR2_model.summary()\n",
    "\n",
    "history = LR2_model.fit(biotite_composition_train, pt_train_norm,\n",
    "                          batch_size=BATCH_SIZE, epochs=MAX_EPOCHS,\n",
    "                          validation_data=[biotite_composition_val, pt_val_norm],\n",
    "                          callbacks=CALLBACKS_LR2, verbose=False)\n",
    "LR2_model.save(Path(\"saved_models\", \"LR2_model_TiXMg\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "lr_schedule.initial_learning_rate = 0.0005"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"sequential_8\"\n",
      "_________________________________________________________________\n",
      " Layer (type)                Output Shape              Param #   \n",
      "=================================================================\n",
      " normalization (Normalizati  (None, 2)                 5         \n",
      " on)                                                             \n",
      "                                                                 \n",
      " dense_20 (Dense)            (None, 16)                48        \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                                                                 \n",
      " dense_21 (Dense)            (None, 1)                 17        \n",
      "                                                                 \n",
      "=================================================================\n",
      "Total params: 70 (284.00 Byte)\n",
      "Trainable params: 65 (260.00 Byte)\n",
      "Non-trainable params: 5 (24.00 Byte)\n",
      "_________________________________________________________________\n",
      "INFO:tensorflow:Assets written to: saved_models\\LR3_model_TiXMg\\assets\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Assets written to: saved_models\\LR3_model_TiXMg\\assets\n"
     ]
    }
   ],
   "source": [
    "OPT = Adam(lr_schedule)\n",
    "\n",
    "LR3_model = Sequential()\n",
    "LR3_model.add(normalisation_biotite_composition)\n",
    "LR3_model.add(Dense(16, activation=\"relu\"))\n",
    "LR3_model.add(Dense(1))\n",
    "\n",
    "LR3_model.compile(optimizer=OPT, loss=LOSS, metrics=METRICS)\n",
    "LR3_model.summary()\n",
    "\n",
    "history = LR3_model.fit(biotite_composition_train, pt_train_norm,\n",
    "                          batch_size=BATCH_SIZE, epochs=MAX_EPOCHS,\n",
    "                          validation_data=[biotite_composition_val, pt_val_norm],\n",
    "                          callbacks=CALLBACKS_LR3, verbose=False)\n",
    "LR3_model.save(Path(\"saved_models\", \"LR3_model_TiXMg\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "lr_schedule.initial_learning_rate = 0.0001"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"sequential_9\"\n",
      "_________________________________________________________________\n",
      " Layer (type)                Output Shape              Param #   \n",
      "=================================================================\n",
      " normalization (Normalizati  (None, 2)                 5         \n",
      " on)                                                             \n",
      "                                                                 \n",
      " dense_22 (Dense)            (None, 16)                48        \n",
      "                                                                 \n",
      " dense_23 (Dense)            (None, 1)                 17        \n",
      "                                                                 \n",
      "=================================================================\n",
      "Total params: 70 (284.00 Byte)\n",
      "Trainable params: 65 (260.00 Byte)\n",
      "Non-trainable params: 5 (24.00 Byte)\n",
      "_________________________________________________________________\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Assets written to: saved_models\\LR4_model_TiXMg\\assets\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Assets written to: saved_models\\LR4_model_TiXMg\\assets\n"
     ]
    }
   ],
   "source": [
    "OPT = Adam(lr_schedule)\n",
    "\n",
    "LR4_model = Sequential()\n",
    "LR4_model.add(normalisation_biotite_composition)\n",
    "LR4_model.add(Dense(16, activation=\"relu\"))\n",
    "LR4_model.add(Dense(1))\n",
    "\n",
    "LR4_model.compile(optimizer=OPT, loss=LOSS, metrics=METRICS)\n",
    "LR4_model.summary()\n",
    "\n",
    "history = LR4_model.fit(biotite_composition_train, pt_train_norm,\n",
    "                          batch_size=BATCH_SIZE, epochs=MAX_EPOCHS,\n",
    "                          validation_data=[biotite_composition_val, pt_val_norm],\n",
    "                          callbacks=CALLBACKS_LR4, verbose=False)\n",
    "LR4_model.save(Path(\"saved_models\", \"LR4_model_TiXMg\"))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "masterproject",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
