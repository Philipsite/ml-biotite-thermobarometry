{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    ":warning:**IMPORTANT NOTICE**:warning:\\\n",
    "*Since the initial parameterisation and gradient descent optimisation are stochastic processes, the training of a neural network is not fully reproducible.*\n",
    "\n",
    "*Therefore, it is not recommended to re-run this script as it will overwrite the original calibration of the neural network used in the work presented here.\n",
    "The purpose of this script is solely to document the training procedure and can be copied as a template to fit other new neural networks.*\n",
    "\n",
    "*To experiment with the models calibrated here, they can be loaded from the `saved_models` directory.*"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# TransferLearning: \"Test different transfering techniques\"\n",
    "\n",
    "3 different ways to transfer prior knowledge from a pre-trained model to the thermobarmeter regression on natural data are tested:\n",
    "- **Feature extraction**: Use the pre-trained model to predict *P* and *T* and use these predictions as additional features for the thermobarmeter regression.\n",
    "- **Fine-tuning**: Use the pre-trained model as intial parameterisation and fine-tune the model on the thermobarmeter regression.\n",
    "\n",
    "    *Different fine-tuning strategies are tested:*\n",
    "    - Fine-tune all layers\n",
    "    - Fine-tune only the 2 last layers\n",
    "    - Fine-tune only the last layer\n",
    "    - Fine-tune with L2 regularization (This will keep weights close to zero, in an additional step a custom regularizer should be implemented to keep the weights close to the prior model weights)\n",
    "\n",
    "(- **Injection learning**: Keep training the prior model, but \"inject\" the natural data.)\n",
    "--> It is to be tested what the best approach to this is. To begin with I would use a 50:50 mix of the prior (simulated) data and the natural data. Could be used in with both feature extraction and fine-tuning. (Maybe test in separate notebook)!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\ThinkPad\\AppData\\Local\\Temp\\ipykernel_27628\\1904154419.py:2: DeprecationWarning: \n",
      "Pyarrow will become a required dependency of pandas in the next major release of pandas (pandas 3.0),\n",
      "(to allow more performant data types, such as the Arrow string type, and better interoperability with other libraries)\n",
      "but was not found to be installed on your system.\n",
      "If this would cause problems for you,\n",
      "please provide us feedback at https://github.com/pandas-dev/pandas/issues/54466\n",
      "        \n",
      "  import pandas as pd\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:From c:\\Users\\ThinkPad\\miniconda3\\envs\\masterproject\\Lib\\site-packages\\keras\\src\\losses.py:2976: The name tf.losses.sparse_softmax_cross_entropy is deprecated. Please use tf.compat.v1.losses.sparse_softmax_cross_entropy instead.\n",
      "\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "import tensorflow as tf\n",
    "\n",
    "from pathlib import Path\n",
    "from keras.models import Model, Sequential, load_model, clone_model\n",
    "from keras.layers import Dense, Normalization, BatchNormalization, LayerNormalization, Dropout, Input, concatenate\n",
    "from keras.losses import MeanSquaredError\n",
    "from keras.optimizers import Adam, schedules\n",
    "from keras.metrics import MeanAbsoluteError, RootMeanSquaredError\n",
    "from keras.callbacks import CSVLogger, EarlyStopping\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "from ml_tb.normalisation import MinMaxScaler\n",
    "from ml_tb.metrics import RMSE_denormalised_T, RMSE_denormalised_P\n",
    "from ml_tb.plot import plot_training_curve, prediction_vs_truth"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Import data, pre-procesing and train/test split\n",
    "\n",
    "Validation set is **20%** of the training set. Approx. 200 samples."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "VALIDATION_FRACTION = 0.2"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Global scaling parameters for MinMaxScaling of the target data are hard-coded to the range of the training data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:From c:\\Users\\ThinkPad\\miniconda3\\envs\\masterproject\\Lib\\site-packages\\keras\\src\\backend.py:873: The name tf.get_default_graph is deprecated. Please use tf.compat.v1.get_default_graph instead.\n",
      "\n"
     ]
    }
   ],
   "source": [
    "scaling_pt = MinMaxScaler(min=[1500, 400], max=[10000, 900], axis=0)\n",
    "inv_scaling_pt = MinMaxScaler(min=[1500, 400], max=[10000, 900], axis=0, invert=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "NaN values in biotite composition:  False\n",
      "NaN values in PT:  False\n"
     ]
    }
   ],
   "source": [
    "# load excel file\n",
    "data = pd.read_excel(Path(\"..\",\"01_fit_natural_biotite\",\"Metapelite-Database_Bt_CLEAN_2024-02-03.xlsx\"))\n",
    "\n",
    "biotite_composition = np.zeros(shape=(len(data), 6))\n",
    "biotite_composition[:, 0] = data[\"Bt-Si\"]\n",
    "biotite_composition[:, 1] = data[\"Bt-Ti\"]\n",
    "biotite_composition[:, 2] = data[\"Bt-Al\"]\n",
    "biotite_composition[:, 3] = data[\"Bt-FeTot\"]\n",
    "biotite_composition[:, 4] = data[\"Bt-Mn\"]\n",
    "biotite_composition[:, 5] = data[\"Bt-Mg\"]\n",
    "\n",
    "pt = np.zeros(shape=(len(data), 2))\n",
    "pt[:, 0] = data[\"Pressure estimate random uniform\"] * 1000 # convert to bar\n",
    "pt[:, 1] = data[\"Temperature random ordered after Ti-in-Bt\"]\n",
    "\n",
    "# check for NaN values (should be already filtered out)\n",
    "print(\"NaN values in biotite composition: \", np.isnan(biotite_composition).any())\n",
    "print(\"NaN values in PT: \", np.isnan(pt).any())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[2.7068577  0.12046875 1.7131128  1.2358406  0.00848252 1.0821007 ]]\n",
      "[[0.04696281 0.04720167 0.09208625 0.22648923 0.00735622 0.25816932]]\n",
      "After scaling, the minimal values of P and T are:  [0.00010304 0.00158875]\n",
      "After scaling, the maximal values of P and T are:  [0.940462  0.8988128]\n"
     ]
    }
   ],
   "source": [
    "# test train split\n",
    "data_train, data_val, pt_train, pt_val = train_test_split(biotite_composition, pt, test_size=VALIDATION_FRACTION, shuffle=True)\n",
    "\n",
    "# NORMALISATION\n",
    "normalisation_biotite_composition = Normalization(axis=-1)\n",
    "normalisation_biotite_composition.adapt(data_train)\n",
    "\n",
    "print(normalisation_biotite_composition.mean.numpy())\n",
    "print(np.sqrt(normalisation_biotite_composition.variance.numpy()))\n",
    "\n",
    "# SCALING of PT\n",
    "pt_train_norm = scaling_pt(pt_train)\n",
    "pt_val_norm = scaling_pt(pt_val)\n",
    "\n",
    "print(\"After scaling, the minimal values of P and T are: \", np.min(pt_train_norm, axis=0))\n",
    "print(\"After scaling, the maximal values of P and T are: \", np.max(pt_train_norm, axis=0))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Set global training parameters for all tests\n",
    "\n",
    "+ Define a custom metric for RMSE_P and RMSE_T"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "def RMSE_P(y_true, y_pred):\n",
    "    return RMSE_denormalised_P(y_true, y_pred, inv_scaling_pt)\n",
    "\n",
    "\n",
    "def RMSE_T(y_true, y_pred):\n",
    "    return RMSE_denormalised_T(y_true, y_pred, inv_scaling_pt)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "BATCH_SIZE = 50\n",
    "STEPS_PER_EPOCH = len(data_train) // BATCH_SIZE\n",
    "MAX_EPOCHS = 5000\n",
    "\n",
    "lr_schedule = schedules.InverseTimeDecay(0.001, decay_steps=STEPS_PER_EPOCH*1000, decay_rate=1, staircase=False)\n",
    "\n",
    "LOSS = MeanSquaredError()\n",
    "METRICS = [MeanAbsoluteError(), RootMeanSquaredError(), RMSE_P, RMSE_T]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Set up callbacks for each test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "CALLBACKS_FEATURE_EXTR = [CSVLogger(\"Transfer_technique_feature_extraction.log\"), EarlyStopping(monitor=\"loss\", patience=50)]\n",
    "CALLBACKS_FINETUNE_NOREG = [CSVLogger(\"Transfer_technique_finetune_noreg.log\"), EarlyStopping(monitor=\"loss\", patience=50)]\n",
    "CALLBACKS_FINETUNE_ALL = [CSVLogger(\"Transfer_technique_finetune_all.log\"), EarlyStopping(monitor=\"loss\", patience=50)]\n",
    "CALLBACKS_FINETUNE_LAST2 = [CSVLogger(\"Transfer_technique_finetune_last2.log\"), EarlyStopping(monitor=\"loss\", patience=50)]\n",
    "CALLBACKS_FINETUNE_LAST = [CSVLogger(\"Transfer_technique_finetune_last.log\"), EarlyStopping(monitor=\"loss\", patience=50)]\n",
    "CALLBACKS_FINETUNE_L2 = [CSVLogger(\"Transfer_technique_finetune_l2.log\"), EarlyStopping(monitor=\"loss\", patience=50)]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Load pre-trained model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:From c:\\Users\\ThinkPad\\miniconda3\\envs\\masterproject\\Lib\\site-packages\\keras\\src\\saving\\legacy\\saved_model\\load.py:107: The name tf.gfile.Exists is deprecated. Please use tf.io.gfile.exists instead.\n",
      "\n",
      "WARNING:tensorflow:From c:\\Users\\ThinkPad\\miniconda3\\envs\\masterproject\\Lib\\site-packages\\keras\\src\\saving\\legacy\\saved_model\\load.py:471: The name tf.logging.warning is deprecated. Please use tf.compat.v1.logging.warning instead.\n",
      "\n",
      "WARNING:tensorflow:Unable to restore custom metric. Please ensure that the layer implements `get_config` and `from_config` when saving. In addition, please use the `custom_objects` arg when calling `load_model()`.\n",
      "WARNING:tensorflow:Unable to restore custom metric. Please ensure that the layer implements `get_config` and `from_config` when saving. In addition, please use the `custom_objects` arg when calling `load_model()`.\n"
     ]
    }
   ],
   "source": [
    "# load a saved model from \"02_pretraining\\saved_models\\model_ds62White2014\"\n",
    "model_prior = load_model(Path(\"..\",\"02_pretraining\", \"saved_models/model_ds62White2014\"), compile=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## **Test 01**: Feature extraction"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"sequential\"\n",
      "_________________________________________________________________\n",
      " Layer (type)                Output Shape              Param #   \n",
      "=================================================================\n",
      " normalization (Normalizati  (None, 6)                 13        \n",
      " on)                                                             \n",
      "                                                                 \n",
      " dense (Dense)               (None, 64)                448       \n",
      "                                                                 \n",
      " dense_1 (Dense)             (None, 64)                4160      \n",
      "                                                                 \n",
      " dense_2 (Dense)             (None, 64)                4160      \n",
      "                                                                 \n",
      " dense_3 (Dense)             (None, 64)                4160      \n",
      "                                                                 \n",
      " dense_4 (Dense)             (None, 2)                 130       \n",
      "                                                                 \n",
      "=================================================================\n",
      "Total params: 13071 (51.06 KB)\n",
      "Trainable params: 0 (0.00 Byte)\n",
      "Non-trainable params: 13071 (51.06 KB)\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "model_prior_01 = clone_model(model_prior, input_tensors=Input(shape=(6,)))\n",
    "# freeze all layers\n",
    "for layer in model_prior_01.layers:\n",
    "    layer.trainable = False\n",
    "\n",
    "model_prior_01.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"model\"\n",
      "__________________________________________________________________________________________________\n",
      " Layer (type)                Output Shape                 Param #   Connected to                  \n",
      "==================================================================================================\n",
      " input_2 (InputLayer)        [(None, 6)]                  0         []                            \n",
      "                                                                                                  \n",
      " normalization (Normalizati  (None, 6)                    13        ['input_2[0][0]']             \n",
      " on)                                                                                              \n",
      "                                                                                                  \n",
      " sequential (Sequential)     (None, 2)                    13071     ['input_2[0][0]']             \n",
      "                                                                                                  \n",
      " concatenate (Concatenate)   (None, 8)                    0         ['normalization[0][0]',       \n",
      "                                                                     'sequential[0][0]']          \n",
      "                                                                                                  \n",
      " dense (Dense)               (None, 16)                   144       ['concatenate[0][0]']         \n",
      "                                                                                                  \n",
      " dense_1 (Dense)             (None, 2)                    34        ['dense[0][0]']               \n",
      "                                                                                                  \n",
      "==================================================================================================\n",
      "Total params: 13262 (51.81 KB)\n",
      "Trainable params: 178 (712.00 Byte)\n",
      "Non-trainable params: 13084 (51.12 KB)\n",
      "__________________________________________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "OPT = Adam(lr_schedule)\n",
    "\n",
    "input_vector = Input(shape=(6,))\n",
    "\n",
    "# predict a prior PT (\"initital guess\") with the prior model\n",
    "prior_PT = model_prior_01(input_vector)\n",
    "\n",
    "# normalise input and concatenate with prior_PT\n",
    "normed_imput = normalisation_biotite_composition(input_vector)\n",
    "concatenated = concatenate([normed_imput, prior_PT])\n",
    "\n",
    "# top model\n",
    "out_dense = Dense(16, activation=\"relu\")(concatenated)\n",
    "out_PT = Dense(2, activation=None)(out_dense)\n",
    "\n",
    "model_feature_extraction = Model(inputs=input_vector, outputs=out_PT)\n",
    "\n",
    "model_feature_extraction.compile(optimizer=OPT, loss=LOSS, metrics=METRICS)\n",
    "model_feature_extraction.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:From c:\\Users\\ThinkPad\\miniconda3\\envs\\masterproject\\Lib\\site-packages\\keras\\src\\utils\\tf_utils.py:492: The name tf.ragged.RaggedTensorValue is deprecated. Please use tf.compat.v1.ragged.RaggedTensorValue instead.\n",
      "\n",
      "WARNING:tensorflow:From c:\\Users\\ThinkPad\\miniconda3\\envs\\masterproject\\Lib\\site-packages\\keras\\src\\engine\\base_layer_utils.py:384: The name tf.executing_eagerly_outside_functions is deprecated. Please use tf.compat.v1.executing_eagerly_outside_functions instead.\n",
      "\n",
      "INFO:tensorflow:Assets written to: saved_models\\feature_extraction\\assets\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Assets written to: saved_models\\feature_extraction\\assets\n"
     ]
    }
   ],
   "source": [
    "history = model_feature_extraction.fit(data_train, pt_train_norm,\n",
    "                    validation_data=(data_val, pt_val_norm),\n",
    "                    batch_size=BATCH_SIZE,\n",
    "                    epochs=MAX_EPOCHS,\n",
    "                    callbacks=CALLBACKS_FEATURE_EXTR,\n",
    "                    verbose=False)\n",
    "\n",
    "model_feature_extraction.save(Path(\"saved_models\", \"feature_extraction\"))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## **Test 02-o**: Fine-tuning all layers without additional regularization (dropout)\n",
    "\n",
    "The \"dumb\" approach. To show that fine-tuning can easily lead to overfitting."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "For fine-tuning tests the learning rate is lowered."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 50% of lr for fine-tuning as intital guess (must be properly tuned later on)\n",
    "lr_schedule = schedules.InverseTimeDecay(0.0005, decay_steps=STEPS_PER_EPOCH*1000, decay_rate=1, staircase=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Build model\n",
    "- Take layers from pre-trained model with trained weights and biases\n",
    "- Add natural data normalization layer at the beginning"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_prior_02o = clone_model(model_prior, input_tensors=Input(shape=(6,)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"sequential\"\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "_________________________________________________________________\n",
      " Layer (type)                Output Shape              Param #   \n",
      "=================================================================\n",
      " normalization (Normalizati  (None, 6)                 13        \n",
      " on)                                                             \n",
      "                                                                 \n",
      " dense (Dense)               (None, 64)                448       \n",
      "                                                                 \n",
      " dense_1 (Dense)             (None, 64)                4160      \n",
      "                                                                 \n",
      " dense_2 (Dense)             (None, 64)                4160      \n",
      "                                                                 \n",
      " dense_3 (Dense)             (None, 64)                4160      \n",
      "                                                                 \n",
      " dense_4 (Dense)             (None, 2)                 130       \n",
      "                                                                 \n",
      "=================================================================\n",
      "Total params: 13071 (51.06 KB)\n",
      "Trainable params: 13058 (51.01 KB)\n",
      "Non-trainable params: 13 (56.00 Byte)\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "OPT = Adam(lr_schedule)\n",
    "\n",
    "model_fine_tune_reg = Sequential()\n",
    "model_fine_tune_reg.add(normalisation_biotite_composition)\n",
    "model_fine_tune_reg.add(model_prior_02o.layers[1])\n",
    "model_fine_tune_reg.add(model_prior_02o.layers[2])\n",
    "model_fine_tune_reg.add(model_prior_02o.layers[3])\n",
    "model_fine_tune_reg.add(model_prior_02o.layers[4])\n",
    "model_fine_tune_reg.add(model_prior_02o.layers[5])\n",
    "\n",
    "model_fine_tune_reg.compile(optimizer=OPT, loss=LOSS, metrics=METRICS)\n",
    "model_fine_tune_reg.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Assets written to: saved_models\\fine_tune_no_reg\\assets\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Assets written to: saved_models\\fine_tune_no_reg\\assets\n"
     ]
    }
   ],
   "source": [
    "history = model_fine_tune_reg.fit(data_train, pt_train_norm,\n",
    "                    validation_data=(data_val, pt_val_norm),\n",
    "                    batch_size=BATCH_SIZE,\n",
    "                    epochs=MAX_EPOCHS,\n",
    "                    callbacks=CALLBACKS_FINETUNE_NOREG,\n",
    "                    verbose=False)\n",
    "\n",
    "model_fine_tune_reg.save(Path(\"saved_models\", \"fine_tune_no_reg\"))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## **Test 02a**: Fine-tuning all layers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_prior_02a = clone_model(model_prior, input_tensors=Input(shape=(6,)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"sequential_1\"\n",
      "_________________________________________________________________\n",
      " Layer (type)                Output Shape              Param #   \n",
      "=================================================================\n",
      " normalization (Normalizati  (None, 6)                 13        \n",
      " on)                                                             \n",
      "                                                                 \n",
      " dense (Dense)               (None, 64)                448       \n",
      "                                                                 \n",
      " dropout (Dropout)           (None, 64)                0         \n",
      "                                                                 \n",
      " dense_1 (Dense)             (None, 64)                4160      \n",
      "                                                                 \n",
      " dropout_1 (Dropout)         (None, 64)                0         \n",
      "                                                                 \n",
      " dense_2 (Dense)             (None, 64)                4160      \n",
      "                                                                 \n",
      " dropout_2 (Dropout)         (None, 64)                0         \n",
      "                                                                 \n",
      " dense_3 (Dense)             (None, 64)                4160      \n",
      "                                                                 \n",
      " dense_4 (Dense)             (None, 2)                 130       \n",
      "                                                                 \n",
      "=================================================================\n",
      "Total params: 13071 (51.06 KB)\n",
      "Trainable params: 13058 (51.01 KB)\n",
      "Non-trainable params: 13 (56.00 Byte)\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "OPT = Adam(lr_schedule)\n",
    "\n",
    "model_fine_tune_all = Sequential()\n",
    "model_fine_tune_all.add(normalisation_biotite_composition)\n",
    "model_fine_tune_all.add(model_prior_02a.layers[1])\n",
    "model_fine_tune_all.add(Dropout(0.2))\n",
    "model_fine_tune_all.add(model_prior_02a.layers[2])\n",
    "model_fine_tune_all.add(Dropout(0.2))\n",
    "model_fine_tune_all.add(model_prior_02a.layers[3])\n",
    "model_fine_tune_all.add(Dropout(0.2))\n",
    "model_fine_tune_all.add(model_prior_02a.layers[4])\n",
    "model_fine_tune_all.add(model_prior_02a.layers[5])\n",
    "\n",
    "model_fine_tune_all.compile(optimizer=OPT, loss=LOSS, metrics=METRICS)\n",
    "model_fine_tune_all.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Assets written to: saved_models\\fine_tune_all\\assets\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Assets written to: saved_models\\fine_tune_all\\assets\n"
     ]
    }
   ],
   "source": [
    "history = model_fine_tune_all.fit(data_train, pt_train_norm,\n",
    "                    validation_data=(data_val, pt_val_norm),\n",
    "                    batch_size=BATCH_SIZE,\n",
    "                    epochs=MAX_EPOCHS,\n",
    "                    callbacks=CALLBACKS_FINETUNE_ALL,\n",
    "                    verbose=False)\n",
    "\n",
    "model_fine_tune_all.save(Path(\"saved_models\", \"fine_tune_all\"))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## **Test 02b**: Fine-tuning only the 2 last layers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_prior_02b = clone_model(model_prior, input_tensors=Input(shape=(6,)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"sequential_2\"\n",
      "_________________________________________________________________\n",
      " Layer (type)                Output Shape              Param #   \n",
      "=================================================================\n",
      " normalization (Normalizati  (None, 6)                 13        \n",
      " on)                                                             \n",
      "                                                                 \n",
      " dense (Dense)               (None, 64)                448       \n",
      "                                                                 \n",
      " dense_1 (Dense)             (None, 64)                4160      \n",
      "                                                                 \n",
      " dense_2 (Dense)             (None, 64)                4160      \n",
      "                                                                 \n",
      " dropout_3 (Dropout)         (None, 64)                0         \n",
      "                                                                 \n",
      " dense_3 (Dense)             (None, 64)                4160      \n",
      "                                                                 \n",
      " dense_4 (Dense)             (None, 2)                 130       \n",
      "                                                                 \n",
      "=================================================================\n",
      "Total params: 13071 (51.06 KB)\n",
      "Trainable params: 8450 (33.01 KB)\n",
      "Non-trainable params: 4621 (18.05 KB)\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "OPT = Adam(lr_schedule)\n",
    "\n",
    "model_fine_tune_last2 = Sequential()\n",
    "model_fine_tune_last2.add(normalisation_biotite_composition)\n",
    "model_fine_tune_last2.add(model_prior_02b.layers[1])\n",
    "model_fine_tune_last2.add(model_prior_02b.layers[2])\n",
    "\n",
    "model_fine_tune_last2.layers[1].trainable = False\n",
    "model_fine_tune_last2.layers[2].trainable = False\n",
    "\n",
    "model_fine_tune_last2.add(model_prior_02b.layers[3])\n",
    "model_fine_tune_last2.add(Dropout(0.2))\n",
    "model_fine_tune_last2.add(model_prior_02b.layers[4])\n",
    "model_fine_tune_last2.add(model_prior_02b.layers[5])\n",
    "\n",
    "model_fine_tune_last2.compile(optimizer=OPT, loss=LOSS, metrics=METRICS)\n",
    "model_fine_tune_last2.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Assets written to: saved_models\\fine_tune_last2\\assets\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Assets written to: saved_models\\fine_tune_last2\\assets\n"
     ]
    }
   ],
   "source": [
    "history = model_fine_tune_last2.fit(data_train, pt_train_norm,\n",
    "                    validation_data=(data_val, pt_val_norm),\n",
    "                    batch_size=BATCH_SIZE,\n",
    "                    epochs=MAX_EPOCHS,\n",
    "                    callbacks=CALLBACKS_FINETUNE_LAST2,\n",
    "                    verbose=False)\n",
    "\n",
    "model_fine_tune_last2.save(Path(\"saved_models\", \"fine_tune_last2\"))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## **Test 02c**: Fine-tuning only the last layer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_prior_02c = clone_model(model_prior, input_tensors=Input(shape=(6,)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"sequential_3\"\n",
      "_________________________________________________________________\n",
      " Layer (type)                Output Shape              Param #   \n",
      "=================================================================\n",
      " normalization (Normalizati  (None, 6)                 13        \n",
      " on)                                                             \n",
      "                                                                 \n",
      " dense (Dense)               (None, 64)                448       \n",
      "                                                                 \n",
      " dense_1 (Dense)             (None, 64)                4160      \n",
      "                                                                 \n",
      " dense_2 (Dense)             (None, 64)                4160      \n",
      "                                                                 \n",
      " dense_3 (Dense)             (None, 64)                4160      \n",
      "                                                                 \n",
      " dense_4 (Dense)             (None, 2)                 130       \n",
      "                                                                 \n",
      "=================================================================\n",
      "Total params: 13071 (51.06 KB)\n",
      "Trainable params: 4290 (16.76 KB)\n",
      "Non-trainable params: 8781 (34.30 KB)\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "OPT = Adam(lr_schedule)\n",
    "\n",
    "model_fine_tune_last = Sequential()\n",
    "model_fine_tune_last.add(normalisation_biotite_composition)\n",
    "model_fine_tune_last.add(model_prior_02c.layers[1])\n",
    "model_fine_tune_last.add(model_prior_02c.layers[2])\n",
    "model_fine_tune_last.add(model_prior_02c.layers[3])\n",
    "\n",
    "\n",
    "model_fine_tune_last.layers[1].trainable = False\n",
    "model_fine_tune_last.layers[2].trainable = False\n",
    "model_fine_tune_last.layers[3].trainable = False\n",
    "\n",
    "model_fine_tune_last.add(model_prior_02c.layers[4])\n",
    "model_fine_tune_last.add(model_prior_02c.layers[5])\n",
    "\n",
    "model_fine_tune_last.compile(optimizer=OPT, loss=LOSS, metrics=METRICS)\n",
    "model_fine_tune_last.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Assets written to: saved_models\\fine_tune_last\\assets\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Assets written to: saved_models\\fine_tune_last\\assets\n"
     ]
    }
   ],
   "source": [
    "history = model_fine_tune_last.fit(data_train, pt_train_norm,\n",
    "                    validation_data=(data_val, pt_val_norm),\n",
    "                    batch_size=BATCH_SIZE,\n",
    "                    epochs=MAX_EPOCHS,\n",
    "                    callbacks=CALLBACKS_FINETUNE_LAST,\n",
    "                    verbose=False)\n",
    "\n",
    "model_fine_tune_last.save(Path(\"saved_models\", \"fine_tune_last\"))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## **Test 02d**: Fine-tuning with L2 regularization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_prior_02d = clone_model(model_prior, input_tensors=Input(shape=(6,)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"sequential_4\"\n",
      "_________________________________________________________________\n",
      " Layer (type)                Output Shape              Param #   \n",
      "=================================================================\n",
      " normalization (Normalizati  (None, 6)                 13        \n",
      " on)                                                             \n",
      "                                                                 \n",
      " dense (Dense)               (None, 64)                448       \n",
      "                                                                 \n",
      " dense_1 (Dense)             (None, 64)                4160      \n",
      "                                                                 \n",
      " dense_2 (Dense)             (None, 64)                4160      \n",
      "                                                                 \n",
      " dense_3 (Dense)             (None, 64)                4160      \n",
      "                                                                 \n",
      " dense_4 (Dense)             (None, 2)                 130       \n",
      "                                                                 \n",
      "=================================================================\n",
      "Total params: 13071 (51.06 KB)\n",
      "Trainable params: 13058 (51.01 KB)\n",
      "Non-trainable params: 13 (56.00 Byte)\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "OPT = Adam(lr_schedule)\n",
    "\n",
    "model_fine_tune_l2 = Sequential()\n",
    "model_fine_tune_l2.add(normalisation_biotite_composition)\n",
    "model_fine_tune_l2.add(model_prior_02d.layers[1])\n",
    "model_fine_tune_l2.layers[1].kernel_regularizer = tf.keras.regularizers.l2()\n",
    "model_fine_tune_l2.layers[1].bias_regularizer = tf.keras.regularizers.l2()\n",
    "model_fine_tune_l2.add(model_prior_02d.layers[2])\n",
    "model_fine_tune_l2.layers[2].kernel_regularizer = tf.keras.regularizers.l2()\n",
    "model_fine_tune_l2.layers[2].bias_regularizer = tf.keras.regularizers.l2()\n",
    "model_fine_tune_l2.add(model_prior_02d.layers[3])\n",
    "model_fine_tune_l2.layers[3].kernel_regularizer = tf.keras.regularizers.l2()\n",
    "model_fine_tune_l2.add(model_prior_02d.layers[4])\n",
    "model_fine_tune_l2.layers[4].kernel_regularizer = tf.keras.regularizers.l2()\n",
    "model_fine_tune_l2.layers[4].bias_regularizer = tf.keras.regularizers.l2()\n",
    "model_fine_tune_l2.add(model_prior_02d.layers[5])\n",
    "model_fine_tune_l2.layers[5].kernel_regularizer = tf.keras.regularizers.l2()\n",
    "model_fine_tune_l2.layers[5].bias_regularizer = tf.keras.regularizers.l2()\n",
    "\n",
    "model_fine_tune_l2.compile(optimizer=OPT, loss=LOSS, metrics=METRICS)\n",
    "model_fine_tune_l2.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<keras.src.regularizers.L2 at 0x1c07b83a650>"
      ]
     },
     "execution_count": 28,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model_fine_tune_l2.layers[1].kernel_regularizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Assets written to: saved_models\\fine_tune_l2\\assets\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Assets written to: saved_models\\fine_tune_l2\\assets\n"
     ]
    }
   ],
   "source": [
    "history = model_fine_tune_l2.fit(data_train, pt_train_norm,\n",
    "                    validation_data=(data_val, pt_val_norm),\n",
    "                    batch_size=BATCH_SIZE,\n",
    "                    epochs=MAX_EPOCHS,\n",
    "                    callbacks=CALLBACKS_FINETUNE_L2,\n",
    "                    verbose=False)\n",
    "\n",
    "model_fine_tune_l2.save(Path(\"saved_models\", \"fine_tune_l2\"))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "masterproject",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
